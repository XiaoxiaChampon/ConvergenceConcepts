---
output: 
  html_document:
    css: "style.css"
runtime: shiny
---

```{js, echo = FALSE}
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
```

```{css, echo = FALSE}
.shiny-frame{height: 850px;}
```  

```{r setup, include=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#add library
library(shiny)
library(shinydashboard)
library(ConvergenceConcepts)
library(ggplot2)
library(tidyr)
library(dplyr)
library(kableExtra)
library(plotly)
```

# Limit Theory {.tabset}

## Motivation & Ideas

**Why do we care about Limit Theory?** By limit theory we mean "what is the behavior our our random variable as some quantity grows?" Usually, we concern ourselves with the sample size ($n$) being the quantity that grows. 

We look at two major ideas:

- Determining \textit{approximate (or large-sample or asymptotic)} distributions.  That is, distributions that can be used when some quantity is 'large' (usually the sample size).
- Understanding whether or not a random variable is observed closer and closer to some quantity as our sample size grows. For instance, the sample mean 'converging' to the population mean $\mu$.

### Motivating Example

A [Pew Research Center survey of 10,701 U.S. adults was conducted in March 2023](https://www.pewresearch.org/science/2023/05/16/americans-largely-positive-views-of-childhood-vaccines-hold-steady/). The survey asked participants questions related to their thoughts on vaccination. One question centered around the perceived efficacy of the MMR vaccine.

<div style = "float:right">
```{r, echo = FALSE, out.width = "400px"}
knitr::include_graphics("img/pew.jpg")
```
</div>

The Center survey finds 88% of Americans say the benefits of childhood vaccines for measles, mumps and rubella (MMR) outweigh the risks, compared with just 10% who say the risks outweigh the benefits.

The sample proportion of 0.88 is an estimate of the population proportion. That is, the actual proportion of U.S. adults that believe the benefits outweigh the risks. 

Of course this is a single number estimate that would change if we sampled again. We can report the standard deviation of this sample proportion, called a standard error, to give us an idea of the variability in the estimate.  

Assuming independence between study participants, we can find an estimated standard error for this sample proportion using techniques learned earlier:

$$\widehat{SE(\hat{p})} = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \approx \sqrt{\frac{0.88*0.12}{10701}} = 0.0031$$
Two big questions arise:

- First, can we provide a range of values we are 'confident' the true proportion falls in?
    + We need to know about more than just the variability of the estimator
    + We need to understand the **distribution** of the estimator!
        - Called the **sampling distribution**
        - Describes the pattern in which we observe this $\hat{p}$
    + The sampling distribution can be difficult to derive in some cases!
- Second, ideally we'd like for our estimator to get closer to the true proportion for larger sample sizes
    + Is there a value that $\hat{p}$ *converges* to as n grows?
    + What does it even mean for a random quantity to converge?
    
These two questions can often be answered by looking at the *limiting* behavior (here as the sample size grows) of the estimator $\hat{p}$.


### Convergence in Distribution Idea

To answer the first question, let's consider determining the sampling distribution through simulation. A distribution just describes the pattern in which we observe our variable. If we can simulate observing the variable, we can create many *realizations* of $\hat{p}$ to understand the sampling distribution. 

To do this we need to make some assumptions. We need to assume we have $n$ $iid$ (independent and identically distributed) trials. We also need to assume a true $p$. Let's use the app below to consider the sampling distribution when $p$ is 0.9 and $n$ is 10.

- After simulating many values, increase the sample size and repeat. As $n$ grows, what happens to the distribution?

Instructions:

- "n: sample size" Slider: Move the  slider to the right to increase the sample size
- "p: true value in population" Slider: Move the  slider to the right to increase the true proportion
- "Generate a sample proportion": Click this button to add a single randomly generated sample proportion to the plot
- "Generate 100 sample proportions": Click this button to add 100 randomly generated sample proportions to the plot
- "Add +/- 2 Standard Error and Overlay Smoothed Density" Check this bot to add bars corresponding to two standard errors and also add a smoothed density overlayed

```{r, echo = FALSE}
shinyApp(
  ui <- fluidPage(
    #inputs on the side for n, p, and generating data
    sidebarPanel(
      sliderInput("sample_size",
                  "n: sample size",
                  min = 1,
                  max = 500,
                  step= 1,
                  value = 100),
      sliderInput("true_p",
                  "p: true value in population",
                  min = 0,
                  max = 1,
                  step= 0.01,
                  value = 0.9),
      actionButton("gen",
                   "Generate a sample proportion"),
      actionButton("gen100",
                   "Generate 100 sample proportions"),
      checkboxInput("bars", 
                    "Add +/- 2 Standard Errors and\nOverlay Smoothed Density",
                    value = FALSE)
    ),
    mainPanel(
      plotOutput("samp_dist")
    )
  ),
  server <- function(input, output, session){
      ys <- reactiveValues(y = c(), n = 0)

      observeEvent(input$gen, 
                   {
                     ys$y <- c(ys$y, rbinom(1, size = input$sample_size, prob = input$true_p))
                     ys$n <- ys$n + 1
                     }
                   )
      
      observeEvent(input$gen100, 
                   {
                     ys$y <- c(ys$y, rbinom(100, size = input$sample_size, prob = input$true_p))
                     ys$n <- ys$n + 100
                     }
       )
      
      observeEvent(c(input$sample_size, input$true_p), 
                   {
                     ys$y <- c()
                     ys$n <- 0
                     }
                   )
            
      output$samp_dist <- renderPlot({
        props <- ys$y/input$sample_size
        if(length(props) > 0) {
          hist(props, 
               xlab = "Sample Proportions", 
               main = paste0("Sampling Distribution of p-hat\n# of sample proportions plotted: ", ys$n),
               freq = FALSE)
          if(input$bars){
            se <- sqrt(input$true_p*(1-input$true_p)/input$sample_size)
            bounds <- c(input$true_p - 2*se, input$true_p + 2*se)
            abline(v = bounds, col = "red", lwd = 2)
            
            text(x = input$true_p + 0.5*se, y = 2, labels = paste0(round(mean(props <= bounds[2] & props >= bounds[1]), 2), " of the distribution\nbetween the bars"))
            lines(density(props, kernel = "gaussian", adjust = 2))
          }
        } else {
          NULL
        }
      })
  }
)
```

As long as the distribution is roughly normal, we can see that 0.95 of the distribution falls within two standard errors of $p$. For 95\% of the $\hat{p}$ values we observe, adding and subtracting two standard errors would capture the true $p$. This means we could use something like 

$$\hat{p}\pm 2*\widehat{SE(\hat{p})}$$

as an interval to *capture* the true $p$. (Indeed this is the usual basic interval for a proportion!)


### Convergence in Probability Idea

To answer the second question, we could consider generating sample proportions for ever increasing values of the sample size and seeing how they behave. Using the app below, we can generate many sample proportions for varying $n$, subtract off the true value of $p$, and see how that difference changes on the plot.

- Start with one sample at each $n$ and see the behavior of $\hat{p}-p$. 
- Now increase the number of sample proportions for each $n$. What aspect of this relationship does this help us understand?
- Increase the sample size you are considering. What happens to the observed difference as $n$ grows?

Instructions:

- "Maximum Sample Size": Enter a number to increase/decrease the largest sample size to consider
- "# samples at each n" Slider: Move the slider to select the number of sample proportions to generate at each given sample size
- "p: true value in population": Move this slider to select the true proportion from the population
- "Create/Update graph": Click this button to create the initial graph or update the graph based off of new selections of the above values


```{r, echo = FALSE}
shinyApp(
  ui <- fluidPage(
    #inputs on the side for n, p, and generating data
    sidebarPanel(
      numericInput("max_size",
                   "Maximum Sample Size",
                   value = 250,
                   min = 10, 
                   max = 5000),
      sliderInput("num_samples",
                  "# samples at each n",
                  min = 1,
                  max = 10,
                  step= 1,
                  value = 1),
      sliderInput("true_p",
                  "p: true value in population",
                  min = 0,
                  max = 1,
                  step= 0.01,
                  value = 0.5),
      actionButton("create",
                   "Create/Update graph")
    ),
    mainPanel(
      plotOutput("convergence")
    )
  ),
  server <- function(input, output, session){
      ps <- reactiveValues(p = c())

      observeEvent(input$create, 
                   {
                     ps$p <- sapply(1:input$max_size, FUN = function(x){
                       rbinom(n = input$num_samples, 
                              size = x,
                              prob = input$true_p)/x
                       })
                     }
                   )
            
      output$convergence <- renderPlot({
        props <- ps$p
        truth <- isolate(input$true_p)
        num <- isolate(input$num_samples)
        max_size <- isolate(input$max_size)
        ep <- 0.05
        if(length(props) == 0) {
          plot(x = NULL, 
               y = NULL, 
               xlim = c(0, max_size), 
               ylim = c(-0.5, 0.5),
               xlab = "Sample Size",
               ylab = "phat-p",
               main = "Plot of sample proportion minus the true proportion\nRed lines indicate +/- 0.05")
          abline(h = c(-ep, ep), col = "red", lwd = 2)
        } else {
          plot(x = rep(1:max_size, each = num), y = c(props)-truth, 
               main = "Plot of sample proportion minus the true proportion\nRed lines indicate +/- 0.05",
               xlab = "Sample Size",
               ylab = "phat - p",
               type = "p")
          abline(h = c(-ep, ep), col = "red", lwd = 2)
        }
      })
  }
)
```

We can see that $\hat{p}-p$ seems to get closer to 0. This indicates that $\hat{p}$ is in some sense *converging* to the true value of $p$!

## Definitions

We have some common language you'll see that we should discuss.

**Random Sample** - $Y_1,...,Y_n$ are a random sample (RS) of size $n$ if they are independent and identically distributed (or $iid$).

This is an extremely common assumption for many methods of inference! 

What does this allow us to know about the joint distribution of 
$Y_1,...,Y_n$?<br><br><br><br>

**Statistic** - Functions of $Y_1,...,Y_n$ from a $RS$ that do not involve unknown parameters.

Examples: 

- $\bar{Y} = \frac{1}{n}\sum_{i=1}^{n}Y_i$
- $S^2 = \frac{1}{n-1}\sum_{i=1}^{n} \left(Y_i-\bar{Y}\right)^2$
- $\mbox{Considering a particular value of }\mu_0, T = \frac{\bar{Y}-\mu_0}{S/\sqrt{n}}$

**Sampling Distribution** - Distribution of a statistic<br><br><br><br>

As we look at convergence in distribution, we'll see many results for approximating distributions. In this case we must be careful to note the difference between an **exact** distribution and an **approximate** (or **large-sample** or **asymptotic**) distribution.

Example: If $Y_i\stackrel{iid}\sim N(\mu,\sigma^2)$ then the \underbar{exact} distribution of $\bar{Y}$ is $\bar{Y}\sim N(\mu,\sigma^2/n)$.  How to prove? Recall $m_Y(t) = e^{\mu t+t^2\sigma^2/2}$.<br><br><br><br><br><br><br><br>  

Theory exists that says if $Y_i\stackrel{iid}\sim f_Y(y)$ where $Var(Y)<\infty$ then $\bar{Y}\stackrel{\bullet}{\sim}N(\mu,\sigma^2/n)$.<br><br><br>

- If we use a exact (or true for any n) distribution for inference, we call it \underline{exact inference}.
- If we use a 'large-sample' (or asymptotic) distribution for inference, we call it \underline{approximate} (or \underline{large-sample} or \underline{asympotic}) \underline{inference}

## Convergence in Distribution

Since RVs are not defined deterministically (we may observe different observations each time we see them!), we can't consider the usual calculus convergence ideas.  Instead we have different types of convergence.  \\

3 major types of convergence of RVs (others exist)
\begin{center}
Almost Sure (with probability 1) $\implies$ Probability $\implies$ Distribution
\end{center}

\textbf{Convergence in Distribution Definition} - If a sequence of RVs 
$Y_1,...,Y_n,...$ with corresponding CDFs $F_{Y_1},...,F_{Y_n},...$ is such 
that 
$$lim_{n\rightarrow\infty}F_{Y_n}(y)=F_Y(y)$$
for some RV $Y$ at all points where $F_Y$ is continuous then 
$Y_n\stackrel{d}\rightarrow Y$.\vspace{1.7in}

Often easier to show convergence of MGFs (if they exist)\\
\textbf{MGF convergence} - If 
$$lim_{n\rightarrow\infty}m_{Y_n}(t) = m_Y(t)$$
for some random variable $Y$ with MGF $m_Y(t)$ then $Y_n\stackrel{d}{\rightarrow}Y$.\\~\\

Do gamma example for sample mean here(exact derived earlier in notes). 

### Convergence in Distribution Exploration

- Suppose that $Y_i\stackrel{iid}\sim Gamma(\alpha, \lambda)$. That is, assume we have a random sample from a Gamma distribution with PDF
$$f_Y(y) = \frac{\lambda^\alpha}{\Gamma(\alpha)}y^{\alpha -1}e^{-\lambda y}$$
with mean $E(Y) = \alpha/\lambda$ and variance $Var(Y) = \alpha/\lambda^2$.
- Consider the statistic $\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i$. 
- The sampling distribution of this statistic can often be well approximated using a Normal distribution. Consider changes to $\alpha$, $\lambda$, and $n$ using the app below. 
- Can you develop a rule of thumb around $\alpha$, $\lambda$, and $n$ for when a Normal distribution may be a reasonable approximation?  
- What to look for in each graph:  
    + Histogram: when does it become a symmetric bell-shape?
    + CDF comparison: We know the result is a Normal distribution here. The graph compares the simulated distribution with a Normal distribution. When do the CDFs essentially overlap?     + Absolute difference in CDFs: When do the differences get small?
- Instructions:
    + "n: sample size": Change this slider to alter the sample sized used to find the sample means
    + "alpha: shape parameter": Change this slide to alter the shape parameter of the population gamma distribution
    + "lambda: scale parameter": Change this slide to alter the scale parameter of the population gamma distribution


```{r, echo=FALSE}
shinyApp(
  ui <- fluidPage(    
    sidebarLayout(
      sidebarPanel( 
        sliderInput("d_ngam",
                    "n: sample size",
                    min = 2,
                    max = 100,
                    step = 1,
                    value = 20,
                    animate=animationOptions(
                      interval = 100,
                      loop = TRUE,
                      playButton = "Play",
                      pauseButton = "Stop")
                    ),
        sliderInput("alpha",
                    "alpha: shape parameter",
                    min = 0.05,
                    max = 5,
                    step = 0.05,
                    value = 1,
                    animate=animationOptions(
                      interval = 100,
                      loop = TRUE,
                      playButton = "Play",
                      pauseButton = "Stop")
                    ) ,
        sliderInput("lambda",
                    "lambda: scale parameter",
                    min = 0.05,
                    max = 5,
                    step= 0.05,
                    value = 1,
                    animate=animationOptions(
                      interval = 100,
                      loop = TRUE,
                      playButton = "Play",
                      pauseButton = "Stop")
                    )
      ),

      mainPanel(
        tabsetPanel(id = "gamma_example",
          tabPanel("Histogram",
                   plotOutput("hist")
                   ),
          tabPanel("CDF Plots",
                   plotOutput("cdf")
                   ),
          tabPanel("Absolute Difference in ECDF with Normal",  
                   actionButton("three_d_plot", "Create/Update 3D Plot"),
                   plotOutput("three_d")
                   )
        )
      )
    )
  ),
  server <- function(input, output, session){
    
    sampled_values <- reactiveValues(gamma_means = c())
    observeEvent(c(input$d_ngam, input$alpha, input$lambda),
                 sampled_values$gamma_means <- replicate(1000, mean(rgamma(n = input$d_ngam, shape = input$alpha, rate = input$lambda)))
                 )
    
    output$hist <- renderPlot({
      d_ngam <- isolate(input$d_ngam)
      data_gamma <- sampled_values$gamma_means
      hist(data_gamma, 
           main = paste0("Distribution of 1000 Observed Sample Means\nWith Sample Size n = ", d_ngam),
           xlab = "Observed Sample Mean",
           freq = FALSE)
          })
    
    output$cdf <- renderPlot({
      d_ngam <- isolate(input$d_ngam)
      d_alpha <- isolate(input$alpha)
      d_lambda <- isolate(input$lambda)
      data_gamma <- sampled_values$gamma_means
      #plot the ecdf of the sample means
      plot(ecdf(data_gamma), 
           main = paste0("Comparison of Empirical CDF to Best\nApproximating Normal CDF with Sample Size n = ", d_ngam),
           col = "red", 
           lwd = 2,
           xlab = "y-bar",
           ylab = "CDF")
      #use theoretical normal here
      gam_mean <- d_alpha/d_lambda
      gam_sd <- sqrt(d_alpha/(d_ngam*d_lambda^2))
      x_plot <- seq(from = gam_mean-3*gam_sd, to = gam_mean+3*gam_sd, length = 500)
      #add the normal's cdf values
      lines(x = x_plot, 
            y = pnorm(x_plot, mean = gam_mean, sd = gam_sd),
            col = "blue", 
            lwd = 2)
      #add legend
      legend('topleft', 
             c(expression(hat(F[bar(Y)](y))),expression(F[X](y))),
             col = c("red", "blue"),
             lwd= 2)
      })

    three_d_plot_values <- reactiveValues(plot_data = c())
    #create data for 3d plot
    observeEvent(input$three_d_plot, {
        d_ngam <- input$d_ngam
        d_alpha <- input$alpha
        d_lambda <- input$lambda
        ns <- 2:d_ngam
        #find means for each sample size
        means <- as.data.frame(
          #to every sample size value 2:n create 100 means of size n
          lapply(ns, 
                 FUN = function(x, d_alpha, d_lambda){replicate(1000, mean(rgamma(n = x, shape = d_alpha, rate = d_lambda)))}, d_alpha = d_alpha, d_lambda = d_lambda), 
          col.names = paste0("n",ns))
        #now find the ecdf for each sample size (each column)
        ecdfs <- apply(X = means, MARGIN = 2, FUN = ecdf)
        #gamma mean and sd
        gam_mean <- d_alpha/d_lambda
        gam_sd <- sqrt(d_alpha/(d_lambda^2*2))
        #create a sequence of ybars to evaluate the ecdf at
        ybars <- seq(from = gam_mean-3*gam_sd, to = gam_mean+3*gam_sd, length = 50)
        #find the corresponding normal distribution's cdf values
        zs <- as.data.frame(
          lapply(ns,
                 FUN = function(x, ybars, d_alpha, d_lambda){
                   pnorm(ybars, 
                         mean = gam_mean, 
                         sd = sqrt(d_alpha/(d_lambda^2*x)))
                   }, ybars = ybars, d_alpha = d_alpha, d_lambda = d_lambda),
          col.names = paste0("n", ns)
        )
        diffs <- lapply(X = 1:length(ecdfs), FUN = function(x) abs(ecdfs[[x]](ybars) - zs[,x]))
        #now we have the ns, ybars, and diffs to plot
        plot_data <- expand.grid(ybars = ybars, ns = ns)
        plot_data$diffs <- unlist(diffs) 
        three_d_plot_values$plot_data <- plot_data
    })
    
    output$three_d <- renderPlot({
        plot_data <- three_d_plot_values$plot_data
        # open3d()
        # plot3d(x = plot_data$ybars, y = plot_data$ns, z = plot_data$diffs)
        # scene <- scene3d()
        # close3d()
        # rglwidget(scene)
        if(length(plot_data)>0){
          print(wireframe(diffs ~ ybars + ns, 
                          data = plot_data, 
                          scales = list(arrows = FALSE), 
                          drape = TRUE, 
                          colorkey = TRUE, 
                          screen = list(z = -50, x = -70), 
                          zlab = list(expression(hat(l)[n] ~ "(x)=|" ~ hat(F)[n] ~ "(x)-" ~ F ~ "(x)|"), rot = 90), 
                          main = "Convergence in Distribution?", 
                          xlab="ybar"))
        } else {
          NULL
        }
    })
  })
```

<!--
\large{\textbf{Details: How to prove convergence in distribution?} 
}\normalsize\\

Example: Let $Y_i\stackrel{iid}{\sim}U(0,1)$, what does 
$Y_{(n)}=max\left\{Y_i\right\}$ converge to in distribution?

\newpage


\footnotesize
\begin{verbatim}
#######################################################################
#Generate many samples of size n from Uniform(0,1), find max for each sample to see the
          distribution of max
n<-c(3,10,100,10000)
N<-20000
maxes<-matrix(,nrow=N,ncol=length(n))

#loop over sample sizes
for (j in 1:length(n)){
  #loop through data sets
  for (i in 1:N){maxes[i,j]<-max(runif(n[j]))}
}

par(mfrow=c(1,length(n)))
hist(maxes[,1],breaks=seq(0,1,by=0.005),main=paste("Hist of Max, n=",n[1],sep=""),xlab="max",
ylab="Dist of max")
hist(maxes[,2],breaks=seq(0,1,by=0.005),main=paste("Hist of Max, n=",n[2],sep=""),xlab="max",
ylab="Dist of max")
hist(maxes[,3],breaks=seq(0,1,by=0.005),main=paste("Hist of Max, n=",n[3],sep=""),xlab="max",
ylab="Dist of max")
hist(maxes[,4],breaks=seq(0,1,by=0.005),main=paste("Hist of Max, n=",n[4],sep=""),xlab="max",
ylab="Dist of max")
\end{verbatim}
\normalsize

\begin{center}
\includegraphics[scale=0.4]{convp1}~\\~\\
\end{center}

Note: 
\textbf{Convergence in distribution to a constant} - If 
$Y_n\stackrel{d}{\rightarrow}c$ then $Y_n\stackrel{p}\rightarrow c$.


Example: Let $Y_i\stackrel{iid}{\sim}U(0,1)$, what is the distribution of 
$W = n(1-Y_{(n)})$ as $n\rightarrow\infty$?  \vspace{4in}

***Very useful result - if $a_1,a_2,...$ converges to $a$ as $n\rightarrow\infty$, then $lim_{n\rightarrow\infty}\left(1+\frac{a_n}{n}\right)^n=e^a$

\newpage

\footnotesize
\begin{verbatim}
#######################################################################

## Convergence in distribution (of w=n(1-max) from U(0,1) to the exp(1))

#Generate many samples of size n from Uniform(0,1), find max for each sample to see the
                           distribution of w=n(1-max)
n<-c(3,10,100,10000)
N<-20000
w<-matrix(,nrow=N,ncol=length(n))

#loop over sample sizes
for (j in 1:length(n)){
  #loop through data sets
  for (i in 1:N){
    w[i,j]<-n[j]*(1-max(runif(n[j])))
  }
}

par(mfrow=c(1,length(n)))
hist(w[,1],main=paste("Hist of W, n=",n[1],sep=""),xlab="max",ylab="Dist of W",prob=T)
seq<-seq(from=0,to=10,by=0.01)
lines(seq,y=dexp(seq))
hist(w[,2],main=paste("Hist of W, n=",n[2],sep=""),xlab="max",ylab="Dist of W",prob=T)
lines(seq,y=dexp(seq))
hist(w[,3],main=paste("Hist of W, n=",n[3],sep=""),xlab="max",ylab="Dist of W",prob=T)
lines(seq,y=dexp(seq))
hist(w[,4],main=paste("Hist of W, n=",n[4],sep=""),xlab="max",ylab="Dist of W",prob=T)
lines(seq,y=dexp(seq))
\end{verbatim}


\textbf{Can also use MGFs for convergence in distribution.}\\

Example: Binomial convergence to a Poisson.  Suppose $Y\sim Bin(n,p)$ where $np \rightarrow \lambda$ as $n$ grows (essentially the mean doesn't change as our sample size increases, implies that $p$ changes as $n$ grows).  Show $Y\stackrel{d}\rightarrow Poi(\lambda)$.\vspace{4.25in}

\footnotesize
\begin{verbatim}
#Binomial convergence to Poisson
#choose n and p combinations so that np is always 5
n <- c(10, 100, 1000)
p <- c(0.5, 0.05, 0.005) 

#number of datasets to create
N <- 100000

set.seed(12)
#create datasets & plot
hist(rbinom(N, size = n[1], prob = p[1]), breaks = seq(-0.5, 10.5, by = 1), prob = T,
xlab = "# of successes", ylab = "PMF value", main = "Comparison of Bin & Poi")
#overlay poisson with mean np
x <- 0:10
lines(x, dpois(x, lambda = n[1]*p[1]), type = "h", col = "blue", lwd = 2)
\end{verbatim}


\newpage


\includegraphics[scale=0.3]{BinPoi1.png}\\
\includegraphics[scale=0.3]{BinPoi2.png}\\
\includegraphics[scale=0.3]{BinPoi3.png}\\

\newpage

\large\textbf{Central Limit Theorem} - \\
\normalsize Let $Y_1,Y_2,...$ be independent with MGF existing (really only need finite variance) and $E(Y_i)=\mu$ and $Var(Y_i)=\sigma^2$ then
$$\frac{\bar{Y}_{n}-\mu}{\sigma/\sqrt{n}}\stackrel{d}{\rightarrow}N(0,1)$$\vspace{1.5in}

\footnotesize
\begin{verbatim}
####################################
#Simulate 10000 samples of size 3,10,50 from N(mu,sigma) distribution
n<-c(3,10,50)
N<-50000
mu<-10
sigma<-3

#list to save data values in
data<-list()
for(i in 1:length(n)){data[[i]]<-matrix(0,nrow=N,ncol=n[i])}

#Create the data
#loop over sample sizes
for (j in 1:length(n)){
  #loop over data sets
  for (i in 1:N){data[[j]][i,]<-rnorm(n=n[j],mean=mu,sd=sigma)}
}

#calculate the z statistic for each sample
means3<-apply(X=data[[1]],FUN=function(data){(mean(data)-mu)/(sigma/sqrt(n[1]))},MARGIN=1)
means10<-apply(X=data[[2]],FUN=function(data){(mean(data)-mu)/(sigma/sqrt(n[2]))},MARGIN=1)
means50<-apply(X=data[[3]],FUN=function(data){(mean(data)-mu)/(sigma/sqrt(n[3]))},MARGIN=1)

hist(means3,main=paste("Histogram of z's with n=",n[1]," from 
N(",mu,",",sigma^2,")",sep=""),prob=T)
lines(seq(from=-3,to=3,by=0.01),dnorm(seq(from=-3,to=3,by=0.01)))
...
\end{verbatim}
\normalsize

\newpage

\begin{center}
\includegraphics[scale=0.65]{CLT1}\\
\includegraphics[scale=0.65]{CLT2}\\
\includegraphics[scale=0.65]{CLT3}
\end{center}

\newpage

\textbf{Proof of CLT} - Goal: Show $m_{\sqrt{n}\frac{\bar{Y}_{n}-\mu}{\sigma}}(t)\rightarrow e^{t^2/2}$ as $n\rightarrow\infty$.


\textbf{CLT Applied to a Sum} - 
$$\sum_{i=1}^{n} Y_i \stackrel{\bullet}{\sim}N(n\mu, n\sigma^2)$$

Example: $Y_1,...,Y_{n}\stackrel{iid}{\sim}f_Y(y)=2y$ if $0<y<1$ (0 O.W.).  
Note: $E(Y)=2/3$ and $Var(Y)=1/18$.\\
Let $S=Y_1+...+Y_n$, can we approximate $P(S\leq 10)$?\vspace{4in}



## Central Limit Theorem Applied to a Sample Proportion

A common application of the CLT is to the sample proportion from a Binomial experiment. If $X_i\stackrel{iid}\sim Bin(1, p)$ with mean $E(Y) = p$ and variance $Var(Y) = p(1-p)$, then define $Y = \sum_{i=1}^{n} X_i$.  We know that $Y\sim Bin(n,p)$. The sample propotion is the average of the $X_i$'s or $Y/n$. We can apply the CLT to obtain a large-sample distribution for the sample proportion:

$$\bar{X} = \frac{\sum_{i=1}^{n}X_i}{n} = \frac{Y}{n}\stackrel{\bullet}\sim N(p, p(1-p)/n)$$

- Try to develop a rule of thumb for when this large-sample Normal distribution can be used.
- You may already know the rules of thumb. If you do, use the app below to see if you can provide an explanation for the rules of thumb.
- What to look for in each graph:  
    + Histogram: when does it become a symmetric bell-shape?
    + CDF comparison: We know the result is a Normal distribution here. The graph compares the simulated distribution with a Normal distribution. When do the CDFs essentially overlap? - Instructions:
    + "n: sample size": Change this slider to alter the sample sized used to find the sample means
    + "p: probability of success": Change this slider to alter the probability of success
    + "M: the number of simulations:" Change this slider to increase or decrease the number of simulated values to plot. The larger the value of M the more accurately the simulations mimic the actual sampling distribution. (Larger values require more computation time though!)
    
```{r, echo=FALSE}
shinyApp(
  ui <- fluidPage(
    sidebarLayout(
      sidebarPanel( 
        sliderInput("d_nprop",
                    "n: sample size",
                    min = 5,
                    max = 100,
                    step = 1,
                    value = 20,
                    animate=animationOptions(
                      interval = 100,
                      loop = TRUE,
                      playButton = "Play",
                      pauseButton = "Stop")
                    ),
        sliderInput("prop",
                    "p: probability of success",
                    min = 0.1,
                    max = 1,
                    step = 0.05,
                    value = 0.1,
                    animate=animationOptions(
                      interval = 1,
                      loop = TRUE,
                      playButton = "Play",
                      pauseButton = "Stop")
                    ) ,
        sliderInput("M",
                    "M: the number of the simulations",
                    min = 50,
                    max = 1000,
                    step= 50,
                    value = 50,
                    animate=animationOptions(
                      interval = 1000,
                      loop = TRUE,
                      playButton = "Play",
                      pauseButton = "Stop")
                    )
      ),

      mainPanel(
        tabsetPanel(id = "proportion_example",
          tabPanel("Histogram",
                   plotOutput("hist")
                   ),
          tabPanel("CDF Plots",
                   plotOutput("cdf")
                   )
        )
      )
    )
  ),
  server <- function(input, output, session){
    
    sampled_values <- reactiveValues(sample_prop = c())
    observeEvent(c(input$d_nprop, input$prop, input$M),
                 sampled_values$sample_prop <- replicate(input$M, (rbinom(input$M,input$d_nprop, input$prop))/input$d_nprop)
                 )
    
    output$hist <- renderPlot({
      d_nprop <- isolate(input$d_nprop)
      data_prop <- sampled_values$sample_prop
      hist(data_prop, 
           main = paste0("Distribution of ",input$M," Observed Sample proportionss\nWith n = ", d_nprop),
           xlab = "Observed Sample Proportion",
           freq = FALSE)
          })
    
    output$cdf <- renderPlot({
      d_nprop <- isolate(input$d_nprop)
      prop <- isolate(input$prop)
      M <- isolate(input$M)
      data_prop <- sampled_values$sample_prop
      #plot the ecdf of the sample means
      plot(ecdf(data_prop), 
           main = paste0("Comparison of Empirical CDF to Best\nApproximating Normal CDF with Sample Size n = ", d_nprop),
           col = "red", 
           lwd = 2,
           xlab = "y/n",
           ylab = "CDF")
      #use theoretical normal here
      prop_mean <- prop
      prop_sd <- sqrt(prop*(1-prop)/d_nprop)
      x_plot <- seq(from = prop_mean-3*prop_sd, to = prop_mean+3*prop_sd, length = 500)
      #add the normal's cdf values
      lines(x = x_plot, 
            y = pnorm(x_plot, mean = prop_mean, sd = prop_sd),
            col = "blue", 
            lwd = 2)
      #add legend
      legend('topleft', 
             c(expression(hat(F[y/n](y))),expression(F[X](y))),
             col = c("red", "blue"),
             lwd= 2)
      })
  })
```


Practically, why is the CLT so important?
\begin{itemize}
	\item Suppose we know $\sigma$ and we want inference for $\mu$.
	\item If we get a RS $Y_1,...,Y_n$ we know 
	$\bar{Y}\stackrel{\bullet}{\sim}N(\mu,\sigma^2/n)$ ($\mu$ only unknown)
	\item We can make an approximate claim about $\mu$ via a confidence interval
	$$P(-1.96<Z<1.96)=0.95 \Leftrightarrow 
	P\left(-1.96<\frac{\bar{Y}-\mu}{\sigma/\sqrt{n}}<1.96\right)=0.95$$
	$$\Leftrightarrow 
	P\left(\bar{Y}-1.96\sigma/\sqrt{n}<\mu<\bar{Y}+1.96\sigma/\sqrt{n}\right)=0.95$$
	\item That is, there is a 95\% probability the RVs 
	$\bar{Y}-1.96\sigma/\sqrt{n}$ and $\bar{Y}+1.96\sigma/\sqrt{n}$ capture 
	$\mu$.
	\item Observe $\bar{Y}=\bar{y}$, get observed CI.  Range we are `confident' 
	contains $\mu$.
	\item Note: No assumption about $Y$'s distribution made other than finite 
	variance!
\end{itemize}

## General Convergence in Distribution Exploration

You may wonder about the behavior of other statistics and other parent populations. The app below allows you to explore the sampling distribution of different statistics.  

- Instructions:
    + "Distribution": Choose the parent population's distribution
        - Parameters corresponding to that distribution will show up below
    + "Statistic": Choose the statistic to calculate, plot on the histogram, and summarize the sampling distribution of
    + "Sample Size (2 if blank)": Set the size of the sample to take from the parent population
    + "Number of Data Sets": Use this slider to set the number of data sets to sample and find the statistic for

```{r, echo = FALSE}
#number of data sets to create
NumData<-10000
shinyApp(
  ui <- fluidPage(
          sidebarLayout(
            sidebarPanel( 
              selectizeInput("dist", 
                             label = "Distribution",
                             selected = "Uniform",
                             choices = sort(c("Uniform", "Normal", "Gamma", "Exponential", "Chi-Square", "Beta", "Binomial", "Geometric", "Poisson", "Cauchy"))),
              numericInput("param1", "Lower Limit", value = 0, step = 1),
              conditionalPanel(condition =  "(input.dist=='Uniform')||(input.dist=='Normal')||(input.dist=='Gamma')||(input.dist=='Beta')||(input.dist=='Cauchy')",
                               numericInput("param2", "Upper Limit", value = 1, step = 1)
                               ),
              selectizeInput("stat", label = "Statistic", selected = "Sample Mean", 
                             choices=c("Sample Mean", "Standardized Sample Mean", "Sample Variance", "Sample Standard Deviation", "Sample Median", "Sample Max", "Sample Min")),
              numericInput("n", label = "Sample Size (2 if blank)", value = 2, min = 2, step = 10, max = 10000),
              sliderInput("N", label = "Number of Data Sets:", min = 1, max = 10000, value = 1, step = 1, animate = list(TRUE, interval = 350, loop = TRUE))
            ),
            mainPanel(
              fluidRow(
                #Show a plot of the parent population  
                column(6, plotOutput("parentPlot")),
              # Show a plot of the sampling distribution
                column(6, plotOutput("samplePlot"))),
              #sampling distribution
              fluidRow(
                column(9, plotOutput("statPlot")),
                column(3, "Summary of Sampling Distribution", tableOutput("sampStatTable"))
              )
            )
          )
        ), 
  server <- function(input, output, session){
      #generate data
  simData<-reactive({
    #get reactive things and inputs
    input$dist
    input$n
    input$param1
    input$param2
        
    #sample size
    n<-input$n
    #Num samples
    dist<-input$dist
        
    if (is.na(n)){n<-2}
    
    #get samples from appropriate distribution 
    if (dist=="Uniform"){
      min<-input$param1
      max<-input$param2
      samples<-matrix(runif(n*NumData,min=min,max=max),nrow=NumData,ncol=n)
    } else if (dist=="Normal") {
      mean<-input$param1
      sd<-input$param2
      samples<-matrix(rnorm(n*NumData,mean=mean,sd=sd),nrow=NumData,ncol=n)
    } else if (dist=="Gamma") {
      alpha<-input$param1
      lambda<-input$param2
      samples<-matrix(rgamma(n*NumData,shape=alpha,rate=lambda),nrow=NumData,ncol=n)
    } else if (dist=="Exponential") {
      lambda<-input$param1
      samples<-matrix(rexp(n*NumData,rate=lambda),nrow=NumData,ncol=n)
    } else if (dist=="Chi-Square") {
      df<-input$param1
      samples<-matrix(rchisq(n*NumData,df=df),nrow=NumData,ncol=n)
    } else if (dist=="Beta") {
      alpha<-input$param1
      beta<-input$param2
      samples<-matrix(rbeta(n*NumData,shape1=alpha,shape2=beta),nrow=NumData,ncol=n)
    } else if (dist=="Binomial") {
      prob<-input$param1
      samples<-matrix(rbinom(n*NumData,size=1,prob=prob),nrow=NumData,ncol=n)
    } else if (dist=="Geometric") {
      prob<-input$param1
      samples<-matrix(rgeom(n*NumData,prob=prob),nrow=NumData,ncol=n)
    } else if (dist=="Poisson") {
      lambda<-input$param1
      samples<-matrix(rpois(n*NumData,lambda=lambda),nrow=NumData,ncol=n)
    } else if (dist=="Cauchy") {
      location<-input$param1
      scale<-input$param2
      samples<-matrix(rcauchy(n*NumData,location=location,scale=scale),nrow=NumData,ncol=n)
    }
    
    #return data
    samples
  })

 
  #update parameters in ui  
  observe({
    #get input
    dist <- input$dist
        
    #update the values of the parameters in the ui based on the distribution
    if (dist=="Uniform"){
      updateNumericInput(session, "param1", label="Lower Limit",value = 0,step=1)
      updateNumericInput(session, "param2", label="Upper Limit",value = 1,step=1)
    } else if (dist=="Normal") {
      updateNumericInput(session, "param1", label="Mean",value = 0,step=1)
      updateNumericInput(session, "param2", label="Standard Deviation",value = 1,min=0,step=1)
    } else if (dist=="Gamma") {
      updateNumericInput(session, "param1", label="Alpha (Shape)",value = 1,min=0,step=1)
      updateNumericInput(session, "param2", label="Lambda (Rate)",value = 1,min=0,step=1)
    } else if (dist=="Exponential") {
      updateNumericInput(session, "param1", label="Lambda (Rate)",value = 1,min=0,step=1)
      updateNumericInput(session, "param2", label="Not Applicable",value = 0,min=0,max=0)
    } else if (dist=="Chi-Square") {
      updateNumericInput(session, "param1", label="DF",value = 1,min=0,step=1)
      updateNumericInput(session, "param2", label="Not Applicable",value = 0,min=0,max=0)
    } else if (dist=="Beta") {
      updateNumericInput(session, "param1", label="alpha",value = 1,min=0,step=1)
      updateNumericInput(session, "param2", label="beta",value = 1,min=0,step=1)
    } else if (dist=="Binomial") {
      updateNumericInput(session, "param1", label="Probability of Success",value = 0.5,min=0,max=1,step=0.05)
      updateNumericInput(session, "param2", label="Not Applicable",value = 0,min=0,max=0)
    } else if (dist=="Geometric") {
      updateNumericInput(session, "param1", label="Probability of Success",value = 0.5,min=0,max=1,step=0.05)
      updateNumericInput(session, "param2", label="Not Applicable",value = 0,min=0,max=0)
    } else if (dist=="Poisson") {
      updateNumericInput(session, "param1", label="Lambda (Mean)",value = 1,min=0,step=1)
      updateNumericInput(session, "param2", label="Not Applicable",value = 0,min=0,max=0)
    } else if (dist=="Cauchy") {
      updateNumericInput(session, "param1", label="Location",value = 0,step=1)
      updateNumericInput(session, "param2", label="Scale",value = 1,min=0,step=1)
    }  

  })

  
  #create parent population plot
  output$parentPlot<-renderPlot({
    #input distribution
    dist<-input$dist
    #sample size
    n<-input$n
    
    #choose appropriate distribution  
    if (dist=="Uniform"){
      #parameters
      min<-input$param1
      max<-input$param2
      #Plotting sequence
      x <- seq(from=min,to=max,length=2)
      #draw the parent distribution plot
      plot(x=x,y=dunif(x=x,min=min,max=max),main=paste(dist," Density"),
           xlab="y", ylab="f(y)",type="l")
    } else if (dist=="Normal") {
      #parameters
      mean<-input$param1
      sd<-input$param2
      #Plotting sequence
      x <- seq(from=mean-4*sd,to=mean+4*sd,length=1000)
      #draw the parent distribution plot
      plot(x=x,y=dnorm(x=x,mean=mean,sd=sd),main=paste(dist," Density"),
           xlab="y", ylab="f(y)",type="l")
    } else if (dist=="Gamma") {
      #parameters
      alpha<-input$param1
      lambda<-input$param2
      #Plotting sequence
      x <- seq(from=0,to=alpha/lambda+4*sqrt(alpha/lambda^2),length=1000)
      #draw the parent distribution plot
      plot(x=x,y=dgamma(x=x,shape=alpha,rate=lambda),main=paste(dist," Density"), xlab="y", ylab="f(y)",type="l")
    } else if (dist=="Exponential") {
      #parameters
      lambda<-input$param1
      #Plotting sequence
      x <- seq(from=0,to=5/lambda,length=1000)
      #draw the parent distribution plot
      plot(x=x,y=dexp(x=x,rate=lambda),main=paste(dist," Density"),xlab="y", ylab="f(y)",type="l")
    } else if (dist=="Chi-Square") {
      #parameters
      df<-input$param1
      #Plotting sequence
      x <- seq(from=0,to=df+4*sqrt(2*df),length=1000)
      #draw the parent distribution plot
      plot(x=x,y=dchisq(x=x,df=df),main=paste(dist," Density"), xlab="y", ylab="f(y)",type="l")
    } else if (dist=="Beta") {
      #Plotting sequence
      x <- seq(from=0,to=1,length=1000)
      #parameters
      alpha<-input$param1
      beta<-input$param2
      #draw the parent distribution plot
      plot(x=x,y=dbeta(x=x,shape1=alpha,shape2=beta),main=paste(dist," Density"), xlab="y", ylab="f(y)",type="l")
    } else if (dist=="Binomial") {
      #Plotting sequence
      x <- seq(from=0,to=n,by=1)
      #parameters
      prob<-input$param1
      #draw the parent distribution plot
      plot(x=x,y=dbinom(x=x,size=n,prob=prob),main=paste(dist," Density"), xlab="y", ylab="f(y)",type="h")
    } else if (dist=="Geometric") {
      #parameters
      prob<-input$param1
      #Plotting sequence
      x<-seq(from=0,to=1/prob+4*sqrt((1-prob)/prob^2)+2,by=1)
      #draw the parent distribution plot
      plot(x=x,y=dgeom(x=x,prob=prob),main=paste(dist," Density"),xlab="y", ylab="f(y)",type="h")
    } else if (dist=="Poisson") {
      #parameters
      lambda<-input$param1
      #Plotting sequence
      x <- seq(from=0,to=lambda+4*sqrt(lambda),by=1)
      #draw the parent distribution plot
      plot(x=x,y=dpois(x=x,lambda=lambda),main=paste(dist," Density"), xlab="y", ylab="f(y)",type="h")
    } else if (dist=="Cauchy") {
      #Plotting sequence
      #parameters
      location<-input$param1
      scale<-input$param2
      x <- seq(from=location-6*scale,to=location+6*scale,length=1000)
      #draw the parent distribution plot
      plot(x=x,y=dcauchy(x=x,location=location,scale=scale),main=paste(dist," Density"), xlab="y", ylab="f(y)",type="l")
    }
    
  })
  
  
  #get plot of sample data for current data set
  output$samplePlot<-renderPlot({
    #input data set number
    N<-input$N
    #sample size
    n<-input$n
    #Get data for data set N
    samples<-simData()
    samples<-samples[N,]

    #Get statistic being used
    statistic<-input$stat

    #create histogram and save it, need to find where stat lies so we can color it
    temp<-hist(samples,main=paste("Histogram of Sample ",N,"'s data"),xlab="Data Values, y",prob=TRUE)    

    #calculate value of statistic
    if ((statistic=="Sample Mean")||(statistic=="Standardized Sample Mean")){
      statval<-mean(samples)
    } else if (statistic=="Sample Variance"){
      statval<-var(samples)
    } else if (statistic=="Sample Standard Deviation"){
      statval<-sd(samples)
    } else if (statistic=="Sample Median"){
      statval<-median(samples)
    } else if (statistic=="Sample Max"){
      statval<-max(samples)
    } else if (statistic=="Sample Min"){
      statval<-min(samples)
    }
        
    #get the break from the histogram where the stat occurs
    index<-which((temp$breaks[1:(length(temp$breaks)-1)]<statval)*(statval<temp$breaks[2:length(temp$breaks)])==1)
    #create the color vector to color the histogram
    clr<-rep("white",length(temp$breaks))
    if ((statistic=="Sample Mean")||(statistic=="Standardized Sample Mean")||(statistic=="Sample Median")||(statistic=="Sample Max")||(statistic=="Sample Min")){
      clr[index]<-"blue"
    }
      #create histogram
      hist(samples,main=paste("Histogram of Sample ",N,"'s data"),xlab="Data Values, y",prob=TRUE,col=clr)    
      
  })

    
  #get histogram of statistic based on data sets so far
  output$statPlot <- renderPlot({
    #sample size
    n<-input$n
    #Num samples
    N<-input$N
    #distribution
    dist<-input$dist
    #stat
    statistic<-input$stat
    
    #figure out what mean and standard deviation of parent are for z-score
    if (dist=="Uniform"){
      min<-input$param1
      max<-input$param2
      meanz<-(min+max)/2
      varz<-(max-min)^2/12
    } else if (dist=="Normal") {
      mean<-input$param1
      sd<-input$param2
      meanz<-mean
      varz<-sd^2
    } else if (dist=="Gamma") {
      alpha<-input$param1
      lambda<-input$param2
      meanz<-alpha/lambda
      varz<-alpha/lambda^2
    } else if (dist=="Exponential") {
      lambda<-input$param1
      meanz<-1/lambda
      varz<-1/lambda^2
    } else if (dist=="Chi-Square") {
      df<-input$param1
      meanz<-df
      varz<-2*df
    } else if (dist=="Beta") {
      alpha<-input$param1
      beta<-input$param2
      meanz<-alpha/(alpha+beta)
      varz<-(alpha*beta)/((alpha+beta)^2*(alpha+beta+1))
    } else if (dist=="Binomial") {
      prob<-input$param1
      meanz<-n*prob
      varz<-n*prob*(1-prob)
    } else if (dist=="Geometric") {
      prob<-input$param1
      meanz<-1/prob
      varz<-(1-prob)/prob^2
    } else if (dist=="Poisson") {
      lambda<-input$param1
      meanz<-varz<-lambda
    } else if (dist=="Cauchy") {
      location<-input$param1
      scale<-input$param2
      meanz<-location
      varz<-scale
    }
    
    #get values of the stat for each sample
    samples<-simData()
    if(N==1){
      if (statistic=="Sample Mean"){
        statvals<-mean(samples[1,])
      } else if (statistic=="Standardized Sample Mean"){
        statvals<-(mean(samples[1,])-meanz)/sqrt(varz/n)
      } else if (statistic=="Sample Variance"){
        statvals<-var(samples[1,])
      } else if (statistic=="Sample Standard Deviation"){
        statvals<-sd(samples[1,])
      } else if (statistic=="Sample Median"){
        statvals<-median(samples[1,])
      } else if (statistic=="Sample Max"){
        statvals<-max(samples[1,])
      } else if (statistic=="Sample Min"){
        statvals<-min(samples[1,])
      }
    } else if (N>1){
      if (statistic=="Sample Mean"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=mean)
      } else if (statistic=="Standardized Sample Mean"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=function(x,meanz,varz){(mean(x)-meanz)/sqrt(varz/n)}, meanz,varz)
      } else if (statistic=="Sample Variance"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=var)
      } else if (statistic=="Sample Standard Deviation"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=sd)
      } else if (statistic=="Sample Median"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=median)
      } else if (statistic=="Sample Max"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=max)
      } else if (statistic=="Sample Min"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=min)
      }
    }

    #get the break from the histogram where the stat for data set N occurs
    temp<-hist(statvals,main=paste("Sampling Distribution of the",statistic),prob=TRUE,xlab="Observed Values") 
    index<-which((temp$breaks[1:(length(temp$breaks)-1)]<statvals[N])*(statvals[N]<temp$breaks[2:length(temp$breaks)])==1)
    #create the color vector to color the histogram
    clr<-rep("white",length(temp$breaks))
    clr[index]<-"blue"
    
    # draw the sampling distribution
    if ((dist=="Cauchy")&&(N>1)&&(N<2000)){
      hist(statvals,main=paste("Sampling Distribution of the",statistic),prob=TRUE,xlab="Observed Values",breaks=100,col=clr) 
    } else {
      hist(statvals,main=paste("Sampling Distribution of the",statistic),prob=TRUE,xlab="Observed Values",col=clr) 
    }

  })

  #get statistics about the sampling distribution  
  output$sampStatTable<-renderTable({
    #sample size
    n<-input$n
    #Num samples
    N<-input$N
    #distribution
    dist<-input$dist
    #stat
    statistic<-input$stat
    
    #figure out what mean and standard deviation of parent are for z-score
    if (dist=="Uniform"){
      min<-input$param1
      max<-input$param2
      meanz<-(min+max)/2
      varz<-(max-min)^2/12
    } else if (dist=="Normal") {
      mean<-input$param1
      sd<-input$param2
      meanz<-mean
      varz<-sd^2
    } else if (dist=="Gamma") {
      alpha<-input$param1
      lambda<-input$param2
      meanz<-alpha/lambda
      varz<-alpha/lambda^2
    } else if (dist=="Exponential") {
      lambda<-input$param1
      meanz<-1/lambda
      varz<-1/lambda^2
    } else if (dist=="Chi-Square") {
      df<-input$param1
      meanz<-df
      varz<-2*df
    } else if (dist=="Beta") {
      alpha<-input$param1
      beta<-input$param2
      meanz<-alpha/(alpha+beta)
      varz<-(alpha*beta)/((alpha+beta)^2*(alpha+beta+1))
    } else if (dist=="Binomial") {
      prob<-input$param1
      meanz<-n*prob
      varz<-n*prob*(1-prob)
    } else if (dist=="Geometric") {
      prob<-input$param1
      meanz<-1/prob
      varz<-(1-prob)/prob^2
    } else if (dist=="Poisson") {
      lambda<-input$param1
      meanz<-varz<-lambda
    } else if (dist=="Cauchy") {
      location<-input$param1
      scale<-input$param2
      meanz<-location
      varz<-scale
    }
      
    #get values of the stat for each sample
    samples<-simData()
    if(N==1){
      if (statistic=="Sample Mean"){
        statvals<-mean(samples[1,])
      }else if (statistic=="Standardized Sample Mean"){
        statvals<-(mean(samples[1,])-meanz)/sqrt(varz)
      } else if (statistic=="Sample Variance"){
        statvals<-var(samples[1,])
      } else if (statistic=="Sample Standard Deviation"){
        statvals<-sd(samples[1,])
      } else if (statistic=="Sample Median"){
        statvals<-median(samples[1,])
      } else if (statistic=="Sample Max"){
        statvals<-max(samples[1,])
      } else if (statistic=="Sample Min"){
        statvals<-min(samples[1,])
      }
    } else if (N>1){
      if (statistic=="Sample Mean"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=mean)
      } else if (statistic=="Standardized Sample Mean"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=function(x,meanz,varz){(mean(x)-meanz)/sqrt(varz/n)}, meanz,varz)
      } else if (statistic=="Sample Variance"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=var)
      } else if (statistic=="Sample Standard Deviation"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=sd)
      } else if (statistic=="Sample Median"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=median)
      } else if (statistic=="Sample Max"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=max)
      } else if (statistic=="Sample Min"){
        statvals<-apply(X=samples[1:N,],MARGIN=1,FUN=min)
      }
    }
    sampstatdisplay<-data.frame(Stats=c("Sample Mean","Sample Var","Sample SD","Sample Min","Sample Median","Sample Max"),Value=c(sprintf("%.4f",mean(statvals)),sprintf("%.4f",var(statvals)),sprintf("%.4f",sd(statvals)),sprintf("%.4f",min(statvals)),sprintf("%.4f",median(statvals)),sprintf("%.4f",max(statvals))))
    
    sampstatdisplay
  },include.colnames=FALSE)
  }
)
```

## Delta Method Normality

\textbf{Large Sample Normality and the Delta Method} - \\
Let $Y_1,Y_2,...$ be a sequence of RVs such that
$$\sqrt{n}(Y_n-\theta_0)\stackrel{d}{\rightarrow}N(0,\sigma^2)~~~~~~or~~~~Y_n\stackrel{\bullet}{\sim}N(\theta_0,\sigma^2/n)$$
For a function g and value $\theta_0$ where $g^{'}(\theta_0)$ exists and is not 0 we have
$$\sqrt{n}(g(Y_n)-g(\theta_0))\stackrel{d}{\rightarrow}N(0,(g^{'}(\theta_0))^2\sigma^2)~~~~~~or~~~~g(Y_n)\stackrel{\bullet}{\sim}N(g(\theta_0),(g^{'}(\theta_0))^2\sigma^2/n)$$
\vspace{4.5in}

Example: Let $Y_i\stackrel{iid}{\sim}Ber(p)$ then $\bar{Y}\stackrel{\bullet}{\sim}N(p,\frac{p(1-p)}{n})$.  Goal: make inference on $\frac{\bar{Y}}{1-\bar{Y}}$.

\newpage

Example:  Suppose $Y_i\stackrel{iid}{\sim}f_Y(y)$ where $E(Y_i)=\mu\neq 0$ and 
$Var(Y_i)=\sigma^2<\infty$.  Goal: make inference on $\frac{1}{\mu}$.  Provide 
an approximate distribution for $1/\bar{Y}$ an \textbf{estimator} of $1/\mu$.


# Convergence in Probability {.tabset}

3 major types of convergence of RVs (others exist)
\begin{center}
Almost Sure (with probability 1) $\implies$ Probability $\implies$ Distribution
\end{center}


\textbf{Convergence in Probability Definition} - A sequence of RVs $Y_1,...,Y_n,...$ converges in probability to a RV $Y$ if for every $\epsilon>0$
$$lim_{n\rightarrow\infty}P(|Y_n-Y|\geq \epsilon)=0 \iff 
lim_{n\rightarrow\infty}P(|Y_n-Y|<\epsilon)=1$$
Denoted by $Y_n\stackrel{p}{\rightarrow}Y$.  \\

We'll mostly care about convergence in probability to a constant, call it $c$.
$$lim_{n\rightarrow\infty}P(|Y_n-c|< \epsilon)=\lim_{n\rightarrow\infty}P(-\epsilon < Y_n-c<\epsilon) =  \lim_{n\rightarrow\infty}P(c-\epsilon < Y_n<c+\epsilon)=1$$


## Convergence in Probability Ideas

Assume that $Y_i\stackrel{iid}\sim N(0,1)$. Let's investigate the behavior of 
$$X = \frac{1}{n^2}\sum_{i=1}^{n}Y_i$$
To put this in the context of the definition, let's refer to $X$ explicitly as a function of $n$:
$$X_n = \frac{1}{n^2}\sum_{i=1}^{n}Y_i$$
We want to understand the behavior of $X_n$ as n grows. We'll see that $X_n\stackrel{p}\rightarrow 0$, which implies that for any $\epsilon>0$ we have
$$\lim_{n\rightarrow\infty}P(-\epsilon < X_n < \epsilon) =0$$

To visualize this, we can consider **sample paths** of $X_n$. That is, we can look at a particular sequence of $y_i$'s that will generate a sequence of $x$ and see how the values change. 

Consider the following 6 values randomly sampled from a $N(0,1)$ and the corresponding sequence of $x_n$ values.
```{r, echo = FALSE}
set.seed(1)
ys <- rnorm(6)
xn <- cumsum(ys)/((1:length(ys))**2)
```

$y$ sequence      | $x$ sequence
------------------|--------------
$y_1$ = `r ys[1]` | $x_1$ = `r ys[1]`$/1^2$ = `r xn[1]`
$y_2$ = `r ys[2]` | $x_2$ = (`r ys[1]`+`r ys[2]`)$/2^2$ = `r xn[2]`
$y_3$ = `r ys[3]` | $x_3$ = (`r ys[1]`+`r ys[2]`+`r ys[3]`)$/3^2$ = `r xn[3]`
$y_4$ = `r ys[4]` | $x_4$ = (`r ys[1]`+...+`r ys[4]`)$/4^2$ = `r xn[4]`
$y_5$ = `r ys[5]` | $x_5$ = (`r ys[1]`+...+`r ys[5]`)$/5^2$ = `r xn[5]`
$y_6$ = `r ys[6]` | $x_6$ = (`r ys[1]`+...+`r ys[6]`)$/6^2$ = `r xn[6]`

If we consider multiple sample paths, then convergence in probability to 0 of this sequence implies that the proportion of sample paths outside of $\pm \epsilon$ should go to zero.

Let's plot our sample path with an $\epsilon = 0.05$:

```{r, echo = FALSE}
epsilon <- 0.05
n <- 6
plot_data <- data.frame(n = 1:n, xn = xn)
ggplot(plot_data, aes(x = n, y = xn)) + 
  geom_line() + 
  ylim(c(-0.75,0.75)) + 
  ggtitle("A sample path of Xn") + 
  geom_abline(intercept = -epsilon, slope = 0, color = "red") + 
  geom_abline(intercept = epsilon, slope = 0, color = "red") + 
  theme(legend.position = "none") +
  scale_x_continuous("n", breaks = 1:n, labels = as.character(1:n))
#plot(x = 1:n, 
#     y = xn, 
#     type = "l", 
#     main = "A sample path of X_n", 
#     ylim = c(-1, 1),
#     xlab = "n",
#     lwd = 2)
#abline(h = c(-epsilon, epsilon), col = "red", lwd = 2)
```

Now let's add 9 more sample paths:

```{r, echo = FALSE, warning = FALSE}
#gen sample path function
get_path <- function(n){
  xn <- cumsum(rnorm(n))/((1:n)^2)
}
M <- 10
set.seed(1)
paths <- replicate(M, get_path(n))
plot_data <- data.frame(xn = c(paths), n = rep(1:n, times = M), path = as.factor(rep(1:M, each = n)))
                    
ggplot(plot_data, aes(x = n, y = xn, color = path)) + 
  geom_line()  + 
  ggtitle("A sample path of Xn") + 
  geom_abline(intercept = -epsilon, slope = 0, color = "red", size = 1.1, lty = "dashed") + 
  geom_abline(intercept = epsilon, slope = 0, color = "red", size = 1.1, lty = "dashed") + 
  annotate("rect", xmin = 3.75, xmax = 4.25, ymin = -0.75, ymax = 0.75, alpha = .2) +
  annotate("text", x = 4, y = 0.8, label = "7/10 fall within\nepsilon bounds", col = "darkblue") + 
  theme(legend.position = "none") +
  scale_x_continuous("n", breaks = 1:n, labels = as.character(1:n))

##add what xiaoxia has below with the bars and proportion business and ggploty this bitch 


# plot(x = 1:n, 
#      y = xn, 
#      type = "l", 
#      main = "A sample path of X_n", 
#      ylim = c(-1, 1),
#      xlab = "n", 
#      lwd = 2)
# for (i in 1:9){
#   lines(x = 1:n, y = paths[,i], lwd = 2)
# }
# abline(h = c(-epsilon, epsilon), col = "red")

```

What we hope to see is that the proportion of lines falling outside of the $\epsilon$ bars goes to 0!


```{r, echo = FALSE, message = FALSE, warning = FALSE}
#get matrix of probabilities for each n
probability <- t(rowSums(abs(paths) > epsilon)/M)
colnames(probability) <- paste0("X", 1:n)
#produce data table and combind probability dataset
table_data <- rbind(t(paths), probability)
rownames(table_data) <- c(paste("Sample Path", 1:M), "Probability")
round(table_data, 2) %>% 
  as.data.frame() %>%
  mutate_if(is.numeric, 
            function(x){
              x = cell_spec(x, 
                            color = ifelse(x >= 0.05, "red", "black"))
              }) %>%
  kable(escape = F, 
        row.names = T, 
        caption = "<center> Probability for 10 Simuations 
                  with sample size 6 </center>", 
        align = "l" ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = T) %>%
  row_spec(11, bold = T, color = "black", background = "lightblue")
```


\large{\textbf{Details:  How do we prove convergence in 
probability?}}\normalsize\\

\textbf{Markov's Inequality} - If $X$ is a nonnegative RV (support has no negative values) for which $E(X)$ exists, then for $t>0$
$$P(X\geq t)\leq \frac{E(X)}{t}$$\vspace{4.25in}

Example: If $X\sim exp(1)$ then $P(X\geq t)=e^{-t}$ and $E(X)/t=1/t$.\\
\includegraphics[scale=1]{markov}\\~\\

\textbf{Chebychev's Inequality} - Let $X$ be a RV with mean = $\mu$ and variance = $\sigma^2$, then for $t>0$ 
$$P(|X-\mu|\geq t)\leq \frac{\sigma^2}{t^2}$$\vspace{1.5in}
\vspace{2.5in}

Example:  If $t=\sigma k$ for $k>0$, we can apply Chebychev's to get
$$P\left(|X-\mu|\geq k\sigma\right)\leq \frac{\sigma^2}{k^2\sigma^2}=\frac{1}{k^2}$$
For $k=2$ we have $P\left(|X-\mu|\geq 2\sigma\right)\leq 1/4$.

\begin{itemize}
\item At least 75\% of a RVs distribution lies within 2 standard deviations of the mean (if these moments exist)
\item Regardless of distribution! (if moments exist)
\item If $X\sim N(\mu,\sigma^2)$ we know $P(|X-\mu|\geq 2\sigma)\approx 0.05$. The bound isn't always very tight!
\end{itemize}

\newpage

\textbf{Relating the inequalities to convergence in probability:}\\

\textbf{Weak Law of Large Numbers (WLLN)} - If $Y_1,Y_2,...$ is a sequence of independent RVs with $E(Y_i)=\mu$, $Var(Y_i)=\sigma^2$ then $\bar{Y}_{n}=\frac{1}{n}\sum_{i=1}^{n}Y_i\stackrel{p}{\rightarrow}\mu$
\vspace{6in}

\textbf{Very powerful result!}
\begin{itemize}
\item Big picture goal is to estimate parameters such as $\mu$.
\item If we get a RS we know that $\bar{Y}$ will be a `close' to $\mu$ for `large' samples.
\end{itemize}
\newpage

Example: $Y_i\stackrel{iid}{\sim}f_Y(y)$ where $E(Y_i^2)$ exists ($E\left(|Y_i^2|\right)<\infty$) then $Y_i^2$ are independent and all have the same expectation!\vspace{3.5in}

Example: If $Y_i\stackrel{iid}\sim f_Y(y)$ with $E(Y)=\mu$ and 
$Var(Y)=\sigma^2$ then\vspace{1in}

\textbf{Continuity Theorem} (works for all three types of convergence) - If 
$Y_1,Y_2,Y_3,...$ converge to $Y$ and $g$ is a continuous function then 
$g(Y_1),g(Y_2),g(Y_3)...$ converge to $g(Y)$. \\

Example: Suppose $Y_i$ are independent each with mean $\mu$, we have interest 
in $\mu^2$.\vspace{1.75in}


Other standard limit results exist such as
$$\mbox{If }Y\stackrel{p}{\rightarrow}\theta, 
X\stackrel{p}{\rightarrow}\lambda\mbox{ then }Y\pm 
X\stackrel{p}{\rightarrow}\theta\pm\lambda$$

\newpage


Example: Consider the `biased' version of the sample variance, $S_n^2=\frac{1}{n}\sum_{i=1}^{n}(Y_i-\bar{Y})^2$.  Let's show $S_n^2\stackrel{p}{\rightarrow}\sigma^2$
\vspace{6.25in}

\textbf{Strategies for proving convergence in probability:}
\begin{itemize}
	\item If we are dealing with averages of iid RVs, apply the LLN!
	\item If we have a function of averages of iid RVs, try the LLN and continuity!
	\item If we don't have an average of iid RVs, resort to the definition of convergence in probability.  Likely try to apply an inequality such as Markov's inequality or Chebychev's inequality.
	\item If we can show $Y_n\stackrel{d}\rightarrow c$ then we know $Y_n\stackrel{p}\rightarrow c$ (later).
\end{itemize}

\newpage
\textbf{Application of the WLLN}

\textbf{Monte Carlo Integration} - Suppose we want to compute a 
\textit{definite} integral but it must be done numerically.  Traditional 
methods: Trapezoid Rule, Simpsons Rule, etc.

Goal: Estimate $I(g)=\int_0^1 g(x)dx$\\

Monte Carlo Integration: - 
\begin{enumerate}
	\item Generate a RS of size $n$ from a $U(0,1)$ distribution ($x_1,...,x_n$)
	\item Evaluate $g$ each observation, $g(x_i)$, and find the average 
\end{enumerate}
Will work as $\frac{1}{n}\sum_{i=1}^{n}g(X_i)\stackrel{p}\rightarrow I(g)$.

\newpage

Example: Let $X\sim N(0,1)$, estimate $P(0<X<1)$.\\

$$P(0<X<1) = \int_0^1 \frac{1}{\sqrt{2\pi}} e^{-x^2/2}dx$$

How to estimate this via MC integration?
\vspace{3in}

\footnotesize
\begin{verbatim}
#X~N(0,1), estimate I(g(x))=P(0<X<1)
#Generate n  values from U(0,1)
n<-10000
x<-runif(n)

#Evaluate all x's at g
gx<-(1/sqrt(2*pi))*exp(-x^2/2)
#Approximate by averaging, which converges to truth by WLLN, i.e. converges to I(g(x))=P(0<X<1)
approximation<-sum(gx)/n
#0.3407

#compare approximation to 'truth'
approximation
#truth
pnorm(q=1)-pnorm(q=0)
#0.3413
\end{verbatim}
\normalsize

\newpage

What if our definite integral is over a different range?\\

Goal: Approximate $I(g)=\int_a^b g(x)dx$
\begin{enumerate}
	\item Generate a RS of size $n$ from a $U(a,b)$
	\item Evaluate each at g(x) and average.  Multiply by appropriate constant.
\end{enumerate}
Works because:\vspace{2in}

Idea can be extended to multivariate integrals:
$$\int_{0}^{9/10}\int_{0}^{1}\int_{0}^{11/10}(4-x^2-y^2-z^2)dzdydx=I$$
	$$E(4-X^2-Y^2-Z^2) = \int_{0}^{9/10}\int_0^1\int_0^{11/10}(4-x^2-y^2-z^2)\left(\frac{1}{9/10-0}\right)\left(\frac{1}{1-0}\right)\left(\frac{1}{11/10-0}\right)dzdydx$$
\begin{enumerate}
	\item Generate $u$ randomly distributed triplets in 
	$(0,9/10)x(0,1)x(0,11/10)$
\item Evaluate at $4-x^2-y^2-z^2$ and average.  Multiply by 
	$(9/10)(1)(11/10)$.
	$$(9/10)(1)(11/10)\frac{1}{n}\sum_{i=1}^{n}(4-x_i^2-y_i^2-z_i^2)\stackrel{p}\rightarrow I$$
\end{enumerate}

\footnotesize
\begin{verbatim}
##########################################
#Estimation of g=4-x^2-y^2-z^2 from z=0,11/10, y=0,1, and x=0,9/10
#generate random values from appropriate ranges 
xyz<-data.frame(x=runif(n,min=0,max=9/10),y=runif(n,min=0,max=1),z=runif(n,min=0,max=11/10))

#evaluate at g
g<-4-xyz$x^2-xyz$y^2-xyz$z^2

#Approximate by averaging with appropriate scaling
(9/10)*(1)*(11/10)*mean(g)

#Truth is 2.9634
\end{verbatim}

### Convergence in Probabilty Exploration

Suppose we have a random sample from a Normal distribution with mean 10 and standard deviation 1. What do you think $W = (\bar{Y})^2$ converges to in probability? Consider the application of the Law of Large Numbers and some educated guess work to use the app below to explore!

- Select the value c that you guess $W$ converges to in probability.
- Choose a sample size to go up to (start smaller and then get larger once you have a good idea).
- Select an $\epsilon$ range.
- Look for the proportion of lines (50 sample paths are generated) falling outside of the $\epsilon$ bars to go to 0!

```{r, echo=FALSE,message=FALSE, eval = TRUE}
#path for data
get_path <- function(n){
  xn <- (cumsum(rnorm(n, mean = 10, sd = 1))/(1:n))^2
}
M <- 50
shinyApp(
  ui <- fluidPage(
    #inputs on the side for n, p, and generating data
    sidebarPanel(
      numericInput("c",
                   "c: Value to converge to",
                   value = 0,
                   min = -500, 
                   max = 500),
      numericInput("n",
                  "n: Sample Size (1 to 1000)",
                  min = 1,
                  max = 1000,
                  step= 1,
                  value = 5),
      sliderInput("epsilon",
                  "epsilon: range",
                  min = 1,
                  max = 10,
                  step= 0.5,
                  value = 10),
      actionButton("create",
                   "Create/Update graph")
    ),
    mainPanel(
      plotlyOutput("convergence")
    )
  ),
  server <- function(input, output, session){
      ps <- reactiveValues(p = c())

      observeEvent(input$create, 
                   {
                    n <- input$n
                     #gen sample path function
                    paths <- replicate(M, get_path(n))
                    plot_data <- data.frame(xn = c(paths), 
                                            n = rep(1:n, times = M), 
                                            path = as.factor(rep(1:M, each = n)))

                    ps$p <- plot_data
                     }
                   )
            
      output$convergence <- renderPlotly({
        plot_data <- ps$p
        c <- isolate(input$c)
        n <- isolate(input$n)
        epsilon <- isolate(input$epsilon)

        if(length(plot_data) == 0) {
          NULL
        } else {
           #add proportion in bounds to data frame
          hover_data <- plot_data %>% 
            group_by(n) %>% 
            summarize(proportion = mean(abs(xn-c) < epsilon))
          if(n < 50){
            p <- ggplot(plot_data, aes(x = n, y = xn)) +
              geom_line(aes(color = path, group = path)) + 
              geom_hline(yintercept = c+epsilon, size = 1, col = "purple") + 
              geom_hline(yintercept = c-epsilon, size = 1, col = "purple") +
              theme_classic() + 
              theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = "none") +
              ggtitle("50 Sample Paths Visualized as the Sample Size Increases") + 
              ylab("Path Values") + 
              scale_x_continuous("n", breaks = 1:n, labels = as.character(1:n))
            p2 <- ggplotly(p)
            for (i in 1:n){
              p2 <- p2 %>% add_polygons(x = c(hover_data$n[i]-0.5, 
                                              hover_data$n[i]-0.5, 
                                              hover_data$n[i]+0.5,
                                              hover_data$n[i]+0.5), 
                                        y = c(-1000, 1000, 1000, -1000), 
                                        line = list(width = 0), 
                                        fillcolor = 'rgba(0, 0, 0, 0)', 
                                        inherit = FALSE, 
                                        name = paste0("Proportion of Sample Paths\n within epsilon bounds (n = ", i, "): ", hover_data$proportion[i]))
            }
          } else {
            p <- ggplot(plot_data, aes(x = n, y = xn)) +
              geom_line(aes(color = path, group = path)) + 
              geom_hline(yintercept = c+epsilon, size = 1, col = "purple") + 
              geom_hline(yintercept = c-epsilon, size = 1, col = "purple") +
              theme_classic() + 
              theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = "none") +
              ggtitle("50 Sample Paths Visualized as the Sample Size Increases") + 
              ylab("Path Values")
            p2 <- ggplotly(p)
            for (i in c(seq(5, n, by = 10), n)){
              p2 <- p2 %>% add_polygons(x = c(hover_data$n[i]-5, 
                                              hover_data$n[i]-5, 
                                              hover_data$n[i]+5,
                                              hover_data$n[i]+5), 
                                        y = c(-500, 500, 500, -500), 
                                        line = list(width = 0), 
                                        fillcolor = 'rgba(0, 0, 0, 0)', 
                                        inherit = FALSE, 
                                        name = paste0("Proportion of Sample Paths\n within epsilon bounds (n = ", i, "): ", hover_data$proportion[i]))
            }
          }
          p2
        }
      })
  }
)


```

Note: 
\textbf{Convergence in distribution to a constant} - If 
$Y_n\stackrel{d}{\rightarrow}c$ then $Y_n\stackrel{p}\rightarrow c$.


%\textbf{Convergence in distribution does not imply convergence in 
%probability!}\\
%
%Consider an example where $X\sim Beta(2,2)$ then $1-X$ will also be 
%distributed as Beta(2,2) (recall the symmetry of the Beta distribution with 
%equal $\alpha$ and $\beta$).\\
%
%Define a sequence of RVs to be $X_n=X$ for all $n$.  Then 
%$X_n\stackrel{d}{\rightarrow}1-X\sim Beta(2,2)$.\\
%
%Now consider convergence in probability, does 
%$X_n\stackrel{p}{\rightarrow}1-X$?  No!\\
%
%To converge in probability we need 
%$$P(|X_n-(1-X)|<\epsilon)=P(|X+X_n-1|<\epsilon)\rightarrow 1$$
%as n goes to infinity for \textbf{every} $\epsilon>0$.  \\
%
%But $X_n$ is defined as $X$, therefore for $\epsilon=1/2$ we have
%\begin{eqnarray*}
%P(|X+X_n-1|<\epsilon)=P(|2X-1|<\epsilon) & = & P((1-\epsilon)/2 < X < 
%(1+\epsilon)/2) < 1\\
%&= &P(1/4 < X < 3/4) <1
%\end{eqnarray*}
%for all $n$.
%
%\newpage
%
%\footnotesize
%\begin{verbatim}
%###########################
%#For a visual display of this, we generate many values from a beta(2,2)
%value<-rbeta(100000,2,2)
%par(mfrow=c(1,2))
%plot(x=seq(from=0,to=1,by=0.01),y=dbeta(seq(from=0,to=1,by=0.01),2,2),type="l",
%                   main="Distribution of X_n for all n",xlab="x",ylab="f_X_n")
%									
%#Now we find |X-(1-X)| and plot this
%hist(abs(value-(1-value)),main="Hist of |X_n-(1-X)| values",xlab="|x_n-(1-x)|")
%\end{verbatim}
%\normalsize
%
%\begin{center}
%\includegraphics[scale=0.3]{convp2}~\\~\\
%\end{center}
%
-->