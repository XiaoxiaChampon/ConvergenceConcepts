---
title: "Convergence Concepts"
html_document:
    css: "style.css"
runtime: shiny
---

```{js, echo = FALSE}
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
```

```{r setup, include=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#add library
library(shiny)
library(shinydashboard)
#library(ConvergenceConcepts)
library(ggplot2)
library(tidyr)
library(dplyr)
library(kableExtra)
library(plotly)
```


# Limit Theory {.tabset}

## Motivation

**Why do we care about Limit Theory?** By limit theory we mean "what is the behavior our our random variable as some quantity grows?" Usually, we concern ourselves with the sample size ($n$) being the quantity that grows. 

We look at two major ideas:

- Determining \textit{approximate (or large-sample or asymptotic)} distributions.  That is, distributions that can be used when some quantity is 'large' (usually the sample size).
- Understanding whether or not a random variable is observed closer and closer to some quantity as our sample size grows. For instance, the sample mean 'converging' to the population mean $\mu$.

### Motivating Example

A [Pew Research Center survey of 10,701 U.S. adults was conducted in March 2023](https://www.pewresearch.org/science/2023/05/16/americans-largely-positive-views-of-childhood-vaccines-hold-steady/). The survey asked participants questions related to their thoughts on vaccination. One question centered around the perceived efficacy of the MMR vaccine.

<div style = "float:right">
```{r, echo = FALSE, out.width = "400px"}
knitr::include_graphics("pew.jpg")
```
</div>

The Center survey finds 88% of Americans say the benefits of childhood vaccines for measles, mumps and rubella (MMR) outweigh the risks, compared with just 10% who say the risks outweigh the benefits.

The sample proportion of 0.88 is an estimate of the population proportion. That is, the actual proportion of U.S. adults that believe the benefits outweigh the risks. 

Of course this is a single number estimate that would change if we sampled again. We can report the standard deviation of this sample proportion, called a standard error, to give us an idea of the variability in the estimate.  

Assuming independence between study participants, we can find an estimated standard error for this sample proportion using techniques learned earlier:

$$\widehat{SE(\hat{p})} = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \approx \sqrt{\frac{0.88*0.12}{10701}} = 0.0031$$
Two big questions arise:

- First, can we provide a range of values we are 'confident' the true proportion falls in?
    + We need to know about more than just the variability of the estimator
    + We need to understand the **distribution** of the estimator!
        - Called the **sampling distribution**
        - Describes the pattern in which we observe this $\hat{p}$
    + The sampling distribution can be difficult to derive in some cases!
- Second, does our estimator $\hat{p}$ get closer to the true value $p$ for larger sample sizes?
    + That is, does $\hat{p}$ *converge* to $p$ as n grows?
    + What does it even mean for a random quantity to converge?
    
These two questions can often be answered by looking at the *limiting* behavior (here as the sample size grows) of the estimator $\hat{p}$.


## $\stackrel{d}{\rightarrow}$ Idea

To answer the first question: "Can we provide a range of values for the parameter?", let's consider determining the *sampling distribution* through simulation. A distribution just describes the pattern in which we observe our variable. If we can simulate observing the variable, we can create many *realizations* of $\hat{p}$ to understand the *sampling distribution*. 

To do this we need to make some assumptions. Namely:

- We have $n$ $iid$ (independent and identically distributed) trials
- A value for the true $p$ 

Let's use the app below to consider the sampling distribution when $p$ is 0.9 and $n$ is 10.

Instructions:

- "n: sample size" Slider: Move the  slider to the right to increase the sample size
- "p: true value in population" Slider: Move the  slider to the right to increase the true proportion
- "Generate a sample proportion": Click this button to add a single randomly generated sample proportion to the plot
- "Generate 100 sample proportions": Click this button to add 100 randomly generated sample proportions to the plot
- "Add +/- 2 Standard Error and Overlay Smoothed Density" Check this bot to add bars corresponding to two standard errors and also add a smoothed density overlayed

```{r, echo = FALSE}
shinyApp(
  ui <- fluidPage(
    #inputs on the side for n, p, and generating data
    sidebarPanel(
      sliderInput("sample_size",
                  "n: sample size",
                  min = 1,
                  max = 500,
                  step= 1,
                  value = 100),
      sliderInput("true_p",
                  "p: true value in population",
                  min = 0,
                  max = 1,
                  step= 0.01,
                  value = 0.9),
      actionButton("gen",
                   "Generate a sample proportion"),
      actionButton("gen100",
                   "Generate 100 sample proportions"),
      checkboxInput("bars", 
                    "Add +/- 2 Standard Errors and\nOverlay Smoothed Density",
                    value = FALSE)
    ),
    mainPanel(
      plotOutput("samp_dist")
    )
  ),
  server <- function(input, output, session){
      ys <- reactiveValues(y = c(), n = 0)

      observeEvent(input$gen, 
                   {
                     ys$y <- c(ys$y, rbinom(1, size = input$sample_size, prob = input$true_p))
                     ys$n <- ys$n + 1
                     }
                   )
      
      observeEvent(input$gen100, 
                   {
                     ys$y <- c(ys$y, rbinom(100, size = input$sample_size, prob = input$true_p))
                     ys$n <- ys$n + 100
                     }
       )
      
      observeEvent(c(input$sample_size, input$true_p), 
                   {
                     ys$y <- c()
                     ys$n <- 0
                     }
                   )
            
      output$samp_dist <- renderPlot({
        props <- ys$y/input$sample_size
        if(length(props) > 0) {
          hist(props, 
               xlab = "Sample Proportions", 
               main = paste0("Sampling Distribution of p-hat\n# of sample proportions plotted: ", ys$n),
               freq = FALSE)
          if(input$bars){
            se <- sqrt(input$true_p*(1-input$true_p)/input$sample_size)
            bounds <- c(input$true_p - 2*se, input$true_p + 2*se)
            abline(v = bounds, col = "red", lwd = 2)
            
            text(x = input$true_p + 0.5*se, y = 2, labels = paste0(round(mean(props <= bounds[2] & props >= bounds[1]), 2), " of the distribution\nbetween the bars"))
            lines(density(props, kernel = "gaussian", adjust = 2))
          }
        } else {
          NULL
        }
      })
  }
)
```

As long as the distribution is roughly normal, we can see that 0.95 of the distribution falls within two standard errors of $p$. For 95\% of the $\hat{p}$ values we observe, adding and subtracting two standard errors would capture the true $p$. This means we could use something like 

$$\hat{p}\pm 2*\widehat{SE(\hat{p})}$$

as an interval to *capture* the true $p$. (Indeed this is the usual basic interval for a proportion!)


## $\stackrel{p}{\rightarrow}$ Idea

To answer the second question:  "does our estimator $\hat{p}$ get closer to the true value $p$ for larger sample sizes?", we could consider generating sample proportions for ever increasing values of the sample size and seeing how they behave. Using the app below, we can generate many sample proportions for varying $n$, subtract off the true value of $p$, and see how that difference changes on the plot.

- Start with one sample proportion at each $n$ and see the behavior of $\hat{p}-p$. 
- Now increase the number of sample proportions generated for each $n$. What aspect of this relationship does this help us understand?
- Increase the sample size you are considering. What happens to the observed difference as $n$ grows?

Instructions:

- "Maximum Sample Size": Enter a number to increase/decrease the largest sample size to consider
- "# samples at each n" Slider: Move the slider to select the number of sample proportions to generate at each given sample size
- "p: true value in population": Move this slider to select the true proportion from the population
- "Create/Update graph": Click this button to create the initial graph or update the graph based off of new selections of the above values


```{r, echo = FALSE}
shinyApp(
  ui <- fluidPage(
    #inputs on the side for n, p, and generating data
    sidebarPanel(
      numericInput("max_size",
                   "Maximum Sample Size",
                   value = 250,
                   min = 10, 
                   max = 5000),
      sliderInput("num_samples",
                  "# samples at each n",
                  min = 1,
                  max = 10,
                  step= 1,
                  value = 1),
      sliderInput("true_p",
                  "p: true value in population",
                  min = 0,
                  max = 1,
                  step= 0.01,
                  value = 0.5),
      actionButton("create",
                   "Create/Update graph")
    ),
    mainPanel(
      plotOutput("convergence")
    )
  ),
  server <- function(input, output, session){
      ps <- reactiveValues(p = c())

      observeEvent(input$create, 
                   {
                     ps$p <- sapply(1:input$max_size, FUN = function(x){
                       rbinom(n = input$num_samples, 
                              size = x,
                              prob = input$true_p)/x
                       })
                     }
                   )
            
      output$convergence <- renderPlot({
        props <- ps$p
        truth <- isolate(input$true_p)
        num <- isolate(input$num_samples)
        max_size <- isolate(input$max_size)
        ep <- 0.05
        if(length(props) == 0) {
          plot(x = NULL, 
               y = NULL, 
               xlim = c(0, max_size), 
               ylim = c(-0.5, 0.5),
               xlab = "Sample Size",
               ylab = "phat-p",
               main = "Plot of sample proportion minus the true proportion\nRed lines indicate +/- 0.05")
          abline(h = c(-ep, ep), col = "red", lwd = 2)
        } else {
          plot(x = rep(1:max_size, each = num), y = c(props)-truth, 
               main = "Plot of sample proportion minus the true proportion\nRed lines indicate +/- 0.05",
               xlab = "Sample Size",
               ylab = "phat - p",
               type = "p")
          abline(h = c(-ep, ep), col = "red", lwd = 2)
        }
      })
  }
)
```

We can see that $\hat{p}-p$ seems to get closer to 0. This indicates that $\hat{p}$ is in some sense *converging* to the true value of $p$!

Now that we some basic intuition, let's formalize what we are talking about.

## Definitions

By *limit*, *large-sample*, or *asymptotic* theory we mean we want to understand the behavior of some quantity, usually a *statistic*, as something changes, usually the sample size $n$. For instance, we will investigate the behavior of the sample mean, $\bar{Y}$, as the sample size grows. We'll look at questions like:

- When the distribution of a statistic (called a **sampling distribution**) is difficult to derive *exactly*, is there a good **approximating** distribution that can be used to get **approximate** probability statements about $\bar{Y}$?
- What value does $\bar{Y}$ *get close to* or *converge to* as the sample size grows? 

Answers to these questions will allows us to do inference (confidence intervals and hypothesis tests) and understand the quality of our estimator.

### Common Assumptions & Definitions

We often make some assumptions about how we observe our random variables in order to investigate these types of questions. For simplicity, we often assume we have a **random sample**.

Random Sample
: $Y_1,..., Y_n$ are a random sample (RS) of size $n$ if the random variables are independent and identically distributed (iid). 

We'll often say 'assume we have a random sample' from some distribution or that 'our random variables are iid' from some distribution. These are equivalent ways of stating this assumption.

For the proportion example mentioned previously, we might formally state our assumption as follows:

- Define $X_i = \begin{cases} 1 & \mbox{if the }i^{th}\mbox{ individual says the benefits of childhood vaccines for MMR outweigh the risks}\\ 0 & \mbox{if not}\end{cases}$
- Then $X_i\stackrel{iid}\sim Ber(p)$ where $p$ represents the true proportion of people in the U.S. that believe the benefits outweight the risks
- The random variable $Y = \sum_{i=1}^{n}X_i \sim Bin(n,p)$
- We then often try to use $Y$ or $\hat{p}=Y/n$ to make inference on $p$. 
- $Y$ and $\hat{p}$ are referred to as **statistics**. Note: $\hat{p}$ is also $\bar{X}$!

Statistic
: A function of $Y_1,Y_2,...,Y_n$ from a random sample that does not involve any unknown parameters is called a statistic.

Commonly studied statistics:  

- $\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i$
- $S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(Y_i-\bar{Y})^2$
- $Y_{(n)} = \mbox{the maximum value from the sample}$

Quantities that aren't statistics:  

- $\frac{\bar{Y}-\mu}{S/\sqrt{n}}$ (since $\mu$ is unknown - if we assume $\mu$ is known (like when we do in a hypothesis test) then this is a statistic)
- $\frac{(n-1)S^2}{\sigma^2}$ (since $\sigma^2$ is unknown)

One type of convergence we'll look at is focused on the pattern in which these statistics are observed, that is, the **sampling distribution** of the statistics.

Recall that a distribution is just the pattern and frequency with which we observe a random variable. With a statistic, we give this distribution the special name of **sampling distribution**. This is because we can think of how that distribution is formed by considering repeated samples from the population, each sample producing the statistic of interest.

Sampling Distribution
: The distribution of a statistic is called a sampling distribution.

```{r, echo = FALSE}
shinyApp(
  ui <- fluidPage(
    #inputs on the side for n, p, and generating data
    sidebarPanel(
      sliderInput("sample_size",
                  "n: sample size",
                  min = 1,
                  max = 500,
                  step= 1,
                  value = 100),
      sliderInput("true_p",
                  "p: true value in population",
                  min = 0,
                  max = 1,
                  step= 0.01,
                  value = 0.9),
      actionButton("gen",
                   "Generate a sample proportion"),
      actionButton("gen100",
                   "Generate 100 sample proportions"),
      checkboxInput("bars", 
                    "Add +/- 2 Standard Errors and\nOverlay Smoothed Density",
                    value = FALSE)
    ),
    mainPanel(
      plotOutput("samp_dist")
    )
  ),
  server <- function(input, output, session){
      ys <- reactiveValues(y = c(), n = 0)

      observeEvent(input$gen, 
                   {
                     ys$y <- c(ys$y, rbinom(1, size = input$sample_size, prob = input$true_p))
                     ys$n <- ys$n + 1
                     }
                   )
      
      observeEvent(input$gen100, 
                   {
                     ys$y <- c(ys$y, rbinom(100, size = input$sample_size, prob = input$true_p))
                     ys$n <- ys$n + 100
                     }
       )
      
      observeEvent(c(input$sample_size, input$true_p), 
                   {
                     ys$y <- c()
                     ys$n <- 0
                     }
                   )
            
      output$samp_dist <- renderPlot({
        props <- ys$y/input$sample_size
        if(length(props) > 0) {
          hist(props, 
               xlab = "Sample Proportions", 
               main = paste0("Sampling Distribution of p-hat\n# of sample proportions plotted: ", ys$n),
               freq = FALSE)
          if(input$bars){
            se <- sqrt(input$true_p*(1-input$true_p)/input$sample_size)
            bounds <- c(input$true_p - 2*se, input$true_p + 2*se)
            abline(v = bounds, col = "red", lwd = 2)
            
            text(x = input$true_p + 0.5*se, y = 2, labels = paste0(round(mean(props <= bounds[2] & props >= bounds[1]), 2), " of the distribution\nbetween the bars"))
            lines(density(props, kernel = "gaussian", adjust = 2))
          }
        } else {
          NULL
        }
      })
  }
)
```

We see that the sampling distribution of $\hat{p}$ looks like a bell curve for some combinations of $n$ and $p$. If we fix a $p$ and increase $n$, we will start to see a bell shape for large enough $n$! Later we'll see that a good **large-sample** distribution for $\hat{p}$ is the Normal distribution with mean $p$ and variance $p(1-p)/n$.  

We can see that there may be a distinction between the *actual* distribution, which is a discrete distribution for $\hat{p}$, and an approximating distribution, the Normal distribution for $\hat{p}$. We call these by different names.

Exact Distribution
: The (sampling) distribution of a quantity that is valid for any sample size (or, occasionally, values of the parameters of the population distribution).

Large-Sample or Approximate Distribution
: A (sampling) distribution that is reasonable to use for a quantity for a *large* sample size (or occasionally other parameter values). 

We use the notation
$$Statistic \stackrel{\bullet}\sim f$$
to denote a large-sample approximating distribution.

In the sample proportion example, we would write

$$\hat{p}\stackrel{\bullet}\sim N(p, p(1-p)/n)$$


## Convergence in Distribution {.tabset}

While convergence in distribution can be visually inspected with a histogram, to formally define **convergence in distribution** we use the cumulative distribution function or CDF.

Recall the Cumulative Distribution Function (or CDF) of a random variable $Y$ is defined as 

$$F_Y(y) = P(Y\leq y)$$
For our binomial example, we can compare the CDF of binomial random variables to Normal random variables to see that the Binomial is 'converging' to the normal distribution in a sense! 

<div style = "float:left">
```{r, echo = FALSE, message = FALSE, out.width = "400px"}
p <- 0.15
n <- 20
plot_seq <- seq(from = -0.21, to = 10, by = 0.01)
NN <- length(plot_seq)
bin_cdf <- pbinom(plot_seq, size = n, prob = p)
norm_cdf <- pnorm(plot_seq, mean = n*p, sd = sqrt(n*p*(1-p)))

ecdf <- data.frame(
  y = c(bin_cdf, norm_cdf),
  CDF = c(rep(paste0("Bin(", n, ", ", p, ")"), NN), rep(paste0("N(", n*p, ", ", round(n*p*(1-p), 2), ")"), NN)),
  x = c(plot_seq, plot_seq)
)

ggplot(ecdf, aes(x = x, y = y, color = CDF)) + 
  geom_line(stat = "identity") +
  theme(text = element_text(size = 15), 
        plot.title = element_text(hjust = 0.5), 
        legend.text = element_text(size=15), 
        legend.position = c(0.8, 0.15)) +
  ggtitle("Binomial vs Normal CDF") +
  ylab("CDF") + 
  xlab("Observed Value")
```
</div>
<div style = "float:right">
```{r, echo = FALSE, message = FALSE,out.width = "400px"}
p <- 0.15
n <- 30
plot_seq <- seq(from = -0.21, to = 25, by = 0.01)
NN <- length(plot_seq)
bin_cdf <- pbinom(plot_seq, size = n, prob = p)
norm_cdf <- pnorm(plot_seq, mean = n*p, sd = sqrt(n*p*(1-p)))

ecdf <- data.frame(
  y = c(bin_cdf, norm_cdf),
  CDF = c(rep(paste0("Bin(", n, ", ", p, ")"), NN), rep(paste0("N(", n*p, ", ", round(n*p*(1-p), 2), ")"), NN)),
  x = c(plot_seq, plot_seq)
)

ggplot(ecdf, aes(x = x, y = y, color = CDF)) + 
  geom_line(stat = "identity") +
  theme(text = element_text(size = 15), 
        plot.title = element_text(hjust = 0.5), 
        legend.text = element_text(size=15), 
        legend.position = c(0.8, 0.15)) +
  ggtitle("Binomial vs Normal") +
  ylab("CDF") + 
  xlab("Observed Value")
```
</div>

<div class = "clearfix"></div>

Convergence in Distribution
: Consider a sequence of random variables $Y_1,...,Y_n,...$ with corresponding CDFs $F_{Y_1}(y), ..., F_{Y_n}(y),..$. Then $Y_n$ converges in distribution to the random variable $Y$ (with CDF $F_Y(y)$) if
$$\lim_{n \rightarrow \infty} F_{Y_n}(y)=F_{Y}(y)$$
or equivalently
$$\lim_{n \rightarrow \infty} |F_{Y_n}(y)-F_{Y}(y)|=0$$
(at all points $y$ where $F_Y(y)$ is continuous). We denote this as
$$Y_n\stackrel{d}\rightarrow Y$$

The subscript $n$ notation here may be confusing. This is just to show the RV on the left is dependent on the sample size in some way. For our example with a Binomial/sample proportion, the distribution clearly depends on $n$. We could write the following to be explicit:

$$Y_n \sim Bin(n, p)\mbox{   and   } \hat{p}_n = Y_n/n$$

We'll prove (via the CLT) that the standardized version of these statistics converge to a standard Normal distribution! For example,
$$Z_n = \frac{\hat{p}_n-p}{\sqrt{p(1-p)/n}} \stackrel{d}{\rightarrow}Z\sim N(0,1)$$

Alternatively, for practical purposes we'll equivalently talk about 'large-sample' distributions using the $\stackrel{\bullet}{\sim}$ notation:
$$\hat{p}_n\stackrel{\bullet}{\sim}N\left(p, \frac{p(1-p)}{n}\right)$$
It is sometimes easier to work with MGFs rather than CDFs. In that case, we can use the following result:

Convergence of MGFs
: Consider a sequence of random variables $Y_1,...,Y_n,...$ with corresponding MGFs $m_{Y_1}(t), ..., m_{Y_n}(t),..$. Then $Y_n$ converges in distribution to the random variable $Y$ (with MGF $m_Y(t)$) if
$$\lim_{n \rightarrow \infty} m_{Y_n}(t) = m_Y(t)$$

<hr style="height:0px;border: none; border-top: 5px solid">

### Proving $\stackrel{d}\rightarrow$ using CDFs

**Example:** Suppose that $Y_i\stackrel{iid}\sim U(0,1)$. That is,
$$f_Y(y) = \begin{cases}1 & 0<y<1\\0 & otherwise\end{cases}$$
and
$$F_Y(y) = \begin{cases} 0 & y < 0\\ y & 0\leq y < 1 \\ 1 & y\geq 1\end{cases}$$
What does the maximum from the sample converge to in distribution as $n$ grows?

Let's generate many samples, find the maximum for each sample, and look at the empirical distribution via a histogram and CDF.

```{r,  echo = FALSE, message = FALSE}
library(tidyr)
M <- 500
#even better way to do this for many n values
ns <- c(10,20,50,100)
maxu <- as.data.frame(
  lapply(ns,
         FUN = function(x, M){replicate(M, max(runif(n = x, 0, 1)))},
         M = M),
         col.names = paste0("X", ns)
                     )
maxu_long <- pivot_longer(maxu, cols = everything(), names_to = "n", values_to = "maximum") %>%
  mutate(Sample_Size = factor(n, levels = paste0("X", ns), labels = paste0("n = ", ns), ordered = TRUE))
ggplot(maxu_long, aes(x=maximum)) +
  geom_histogram(bins = 50)+
  facet_wrap( ~ Sample_Size,ncol = 2)+
  theme(text = element_text(size = 15),
        plot.title = element_text(hjust = 0.5),
        legend.text = element_text(size=15),
        legend.position = c(0.1, 0.85)) +
  ggtitle("Distribution of Maximum of uniform (0, 1)") +
  ylab("Frequency") +
  xlab("Observed Maximum Value")


ggplot(maxu_long, aes(x=maximum)) +
  stat_ecdf()+
  facet_wrap( ~ Sample_Size,ncol = 2)+
  theme(text = element_text(size = 15),
        plot.title = element_text(hjust = 0.5),
        legend.text = element_text(size=15),
        legend.position = c(0.1, 0.85)) +
  ggtitle("Distribution of Maximum of uniform (0, 1)") +
  ylab("Empirical CDF") +
  xlab("Observed Maximum Value")
```

It appears that the distribution converges to a random variable that always takes on 1. We'd say there is a **point mass** at 1. If $W$ is a random variable that always takes on the constant $c$ then 
$$f_W(w) = \begin{cases} 1 & w = c\\ 0 & otherwise\end{cases}$$
$$F_W(w) = \begin{cases} 0 & w < c\\ 1 & w\geq c\end{cases}$$

We also said we could look at 
$$\lim_{n \rightarrow \infty} |F_{Y_n}(y)-F_{Y}(y)|=0$$
for convergence in distribution. This difference in CDFs can be plotted in three dimensions.


```{r, echo = FALSE}
ns <- c(1:50)*4
maxs <- as.data.frame(
  lapply(ns, 
         FUN = function(x){
           replicate(100, max(runif(n = x)))
         }
  ),
  col.names = paste0("n",ns))

#now find the ecdf for each sample size (each column)
ecdfs <- apply(X = maxs, MARGIN = 2, FUN = ecdf)

#find the corresponding point mass at 1 values to compare against
max_values <- 1:100/100
pms <- c(rep(0, 99), 1)
diffs <- lapply(X = 1:length(ecdfs), FUN = function(x) abs(ecdfs[[x]](max_values) - pms))
#now we have the ns, ybars, and diffs to plot
plot_data <- expand.grid(max_values = max_values, ns = ns)
plot_data$diffs <- unlist(diffs) 
lattice::wireframe(diffs ~ max_values + ns, 
                          data = plot_data, 
                          scales = list(arrows = FALSE), 
                          drape = TRUE, 
                          colorkey = TRUE, 
                          zlab = list(expression(hat(l)[n] ~ "(x)=|" ~ hat(F)[n] ~ "(x)-" ~ F ~ "(x)|"), rot = 90), 
                          main = "Convergence in Distribution?", 
                          xlab="max")
```

Let's formally prove that the maximum of $n$ iid $U(0,1)$ RVs converges to a RV with a point mass at 1.
<br><br><br>

**Example:** Consider again a random sample of $U(0,1)$ RVs. What does $W = n(1-Y_{(n)})$ converge in distribution to as $n$ grows? Can we describe a rule of thumb for when the approximating distribution is reasonable?

```{r,message=FALSE,echo=FALSE}
#W = n(1-Y_{(n)})
M <- 500
#even better way to do this for many n values
ns <- c(5,10,25,100)
maxu <- as.data.frame(
  lapply(ns,
         FUN = function(x, M){replicate(M, x*(1-max(runif(n = x, 0, 1))))},
         M = M),
  col.names = paste0("X",ns)
                      )
maxu_long <- pivot_longer(maxu, cols = everything(), names_to = "n", values_to = "W") %>%
  mutate(Sample_Size = factor(n, levels = paste0("X", ns), labels = paste0("n = ", ns), ordered = TRUE))

ggplot(maxu_long, aes(x=W)) +
  geom_histogram(bins = 50)+
  facet_wrap( ~ Sample_Size, ncol = 2)+
  theme(text = element_text(size = 15),
        plot.title = element_text(hjust = 0.5),
        legend.text = element_text(size=15),
        legend.position = c(0.1, 0.85)) +
  ggtitle("Distribution of n*(1-max) from U(0,1)") +
  ylab("Frequency") +
  xlab("Observed Maximum Value")


ggplot(maxu_long, aes(x = W)) +
  stat_ecdf()+
  facet_wrap( ~ Sample_Size,ncol = 2)+
  theme(text = element_text(size = 15),
        plot.title = element_text(hjust = 0.5),
        legend.text = element_text(size=15),
        legend.position = c(0.1, 0.85)) +
  ggtitle("Distribution of n*(1-max) from U(0,1)") +
  ylab("Empirical CDF") +
  xlab("Observed Maximum Value")

```

This one doesn't appear to be converging to a point mass. Let's use the limit of the CDF to determine what this random variable converges to.
<br><br><br>


Let's compare the distribution of $W = n(1-Y_{(n)})$ with $X \sim exp(1)$ via a plot of 
$$|F_{Y_n}(y)-F_{Y}(y)|$$

```{r, echo = FALSE}
ns <- c(1:100)
maxs <- as.data.frame(
  lapply(ns, 
         FUN = function(x, M){replicate(500, x*(1-max(runif(n = x, 0, 1))))}),
  col.names = paste0("n",ns))

#now find the ecdf for each sample size (each column)
ecdfs <- apply(X = maxs, MARGIN = 2, FUN = ecdf)

#find the corresponding point mass at 1 values to compare against
max_values <- 1:50/10
exp_cdfs <- pexp(max_values, rate = 1)
diffs <- lapply(X = 1:length(ecdfs), FUN = function(x) abs(ecdfs[[x]](max_values) - exp_cdfs))
#now we have the ns, ybars, and diffs to plot
plot_data <- expand.grid(max_values = max_values, ns = ns)
plot_data$diffs <- unlist(diffs) 
lattice::wireframe(diffs ~ max_values + ns, 
                          data = plot_data, 
                          scales = list(arrows = FALSE), 
                          drape = TRUE, 
                          colorkey = TRUE, 
                          zlab = list(expression(hat(l)[n] ~ "(x)=|" ~ hat(F)[n] ~ "(x)-" ~ F ~ "(x)|"), rot = 90), 
                          main = "Convergence in Distribution?", 
                          xlab="W")
```


### Proving $\stackrel{d}\rightarrow$ using MGFs

**Example:** Suppose $Y\sim Bin(n,p)$ where $np \rightarrow \lambda$ as $n$ grows.  Show $Y\stackrel{d}\rightarrow Poi(\lambda)$. 

First, let's compare plots to see that the relationship seems to hold. We'll create three different binomial and poisson plots with the same ratio for $n$ and $p$. 

Consider how well do the PMFs match up for the following situations:

```{r, echo = FALSE}
#Binomial convergence to Poisson
#choose n and p combinations so that np is always 5
n <- c(10, 100, 1000)
p <- c(0.5, 0.05, 0.005) 

#number of datasets to create
N <- 100000
x <- 0:17
```

- $n = 10, p = 0.5 \rightarrow n*p = 5$

```{r echo = FALSE}

i <- 1
plot(x, 
     y = dbinom(x,
                size = n[i], 
                prob = p[i]), 
     type = "h",
     col = "red",
     lwd = 2,
     xlab = "# of successes", 
     ylab = "PMF value", 
     main = paste0("Bin/Poi: n=", n[i], ", p=", p[i]))
    #overlay poisson with mean np
lines(x + 0.2, 
      dpois(x, lambda = n[i]*p[i]), 
      type = "h", 
      col = "blue", 
      lwd = 2)
legend(x = "topright", legend = c("Binomial", "Poisson"), col = c("red", "blue"), lwd = 2)
```

- $n = 100, p = 0.05 \rightarrow n*p = 5$

```{r echo = FALSE}

i <- 2
plot(x, 
     y = dbinom(x,
                size = n[i], 
                prob = p[i]), 
     type = "h",
     col = "red",
     lwd = 2,
     xlab = "# of successes", 
     ylab = "PMF value", 
     main = paste0("Bin/Poi: n=", n[i], ", p=", p[i]))
    #overlay poisson with mean np
lines(x + 0.2, 
      dpois(x, lambda = n[i]*p[i]), 
      type = "h", 
      col = "blue", 
      lwd = 2)
legend(x = "topright", legend = c("Binomial", "Poisson"), col = c("red", "blue"), lwd = 2)
```

- $n = 1000, p = 0.005 \rightarrow n*p = 5$

```{r echo = FALSE}
i <- 3
plot(x, 
     y = dbinom(x,
                size = n[i], 
                prob = p[i]), 
     type = "h",
     col = "red",
     lwd = 2,
     xlab = "# of successes", 
     ylab = "PMF value", 
     main = paste0("Bin/Poi: n=", n[i], ", p=", p[i]))
    #overlay poisson with mean np
lines(x + 0.2, 
      dpois(x, lambda = n[i]*p[i]), 
      type = "h", 
      col = "blue", 
      lwd = 2)
legend(x = "topright", legend = c("Binomial", "Poisson"), col = c("red", "blue"), lwd = 2)
```


<hr>

**Example:** After we learn about the central limit theorem (CLT), we'll see a (relatively) easy way to prove that a $Y\sim Gamma(n, \lambda)$, properly standardized, converges to a standard normal distribution.  That is, 
$$W = \frac{Y-n/\lambda}{\sqrt{n}/\lambda}\stackrel{d}{\rightarrow} Z \sim N(0,1)$$
We can prove it using MGFs directly. Let's look through the proof below:

Goal: Start with the MGF of the standardized random variable and try to show it converges to a standard 
normal MGF ($e^{t^2/2}$) as $n\rightarrow\infty$.


\begin{align*} 
m_Z(t) &= E\left(e^{t\left(\frac{Y-n/\lambda}{\sqrt{n}/\lambda}\right)}\right)\\
       &= E\left(e^{\frac{t\lambda}{\sqrt{n}}Y}\right)e^{-t\sqrt{n}}\\
       &= \left(\frac{1}{1-\frac{(\lambda t)/\sqrt{n}}{\lambda}}\right)^{n}e^{-t\sqrt{n}}\\
       &= \left(\frac{e^{-t/\sqrt{n}}}{1-t/\sqrt{n}}\right)^{n}\\
\end{align*}

As we want the limit of this quantity as $n$ goes to infinity, consider that this involves $n$ in the term and is raised to the $n$. We saw the result:

$$\lim_{n\rightarrow\infty}(1+a_n/n)^n=e^a$$
where $\lim_{n\rightarrow\infty}a_n=a$.  A rewrite can allow us to use this!

\begin{align*}
      &= \left(1+\frac{n\left(\frac{e^{-t/\sqrt{n}}}{1-t/\sqrt{n}}-1\right)}{n}\right)^{n}\\
\end{align*}

Now we can just consider what happens to the numerator of the second term as 
$n$ grows.  That is, we just need to consider
$$\lim_{n\rightarrow\infty}n\left(\frac{e^{-t/\sqrt{n}}}{1-t/\sqrt{n}}-1\right)$$
Using a common denominator and then applying a Taylor series expansion of the 
$e$ term about 0,
$$e^{-t/\sqrt{n}}=1-t/\sqrt{n}+t^2/(2n)-t^3/(3!n^{3/2})+...,$$
we can rewrite this as
\begin{align*}
    &= \lim_{n\rightarrow\infty}n\left(\frac{t^2/(2n)-t^3/(3!n^{3/2})+...}{1-t/\sqrt{n}}\right)\\
    &= \lim_{n\rightarrow\infty}\left(\frac{t^2/2-t^3/(3!n^{1/2})+...}{1-t/\sqrt{n}}\right)\\
    &= t^2/2\\
\end{align*}

Thus, our MGF converges $e^{t^2/2}$. This is the MGF of a standard normal random variable! Therefore,

$$W = \frac{Y-n/\lambda}{\sqrt{n/\lambda^2}}\stackrel{d}{\rightarrow} Z \sim N(0,1)$$


### Central Limit Theorem

One of the most important theorems in statistics is the Central Limit Theorem (CLT). The CLT gives us a general result about the large-sample behavior of a sample mean.

Central Limit Theorem (CLT)
: Suppose that $Y_i\stackrel{iid}\sim f_Y$ where $E(Y)=\mu$ and $Var(Y)=\sigma^2 < \infty$.  Define $\bar{Y}=\frac{1}{n} \sum_{i=1}^{n} Y_i$ and $Z \sim N(0, 1)$. Then the standardized sample mean converges in distribution to a standard normal random variable.
$$\frac{\bar{Y}-\mu}{\sigma/\sqrt{n}} \stackrel {d} {\rightarrow} Z$$

Practically, we can say that a good approximating distribution or large-sample distribution for $\bar{Y}$ is
$$\bar{Y}\stackrel{\bullet}\sim N(\mu, \sigma^2/n)$$


#### CLT Applied to a Sample Proportion

**Example:** A common application of the CLT is to the sample proportion from a Binomial experiment.

For example, if $X_i\stackrel{iid}\sim Bin(1, p)$ with mean $E(Y) = p$ and variance $Var(Y) = p(1-p)$. 

Define $Y = \sum_{i=1}^n X_i$. We know $Y\sim Bin(n, p)$. The sample proportion is then
$$\hat{p}=\frac{Y}{n} = \frac{\sum_{i=1}^n X_i}{n}$$ 
By the CLT, we can say that the sampling distribution of $\hat{p}$ can be well approximated by a Normal distribution with mean $E(X_i) = p$ and variance $Var(X_i)/n = p(1-p)/n$.

We might state this as either 

$$\frac{\hat{p}-p}{\sqrt{p(1-p)/n}} \stackrel{d}\rightarrow Z\sim N(0,1)$$
or 

$$\hat{p}\stackrel{\bullet}\sim N(p, p(1-p)/n)$$

You've likely seen the Normal approximation to the Binomial before. You may even know a rule of thumb for using it. The app below simulates sample proportions from a given binomial distribution. Use the app below to 

- Explore the relationship between $\hat{p}$ and the corresponding Normal distribution (the larger the $M$ value the more precisely the graph mimics the exact distribution of $\hat{p}$)
- Either verify the rule of thumb you know or try and come up with a rule of thumb for when the approximation is reasonable

```{css, echo = FALSE}
.shiny-frame{height: 450px;}
``` 

```{r, echo=FALSE}
shinyApp(
  ui <- fluidPage(
    sidebarLayout(
      sidebarPanel( 
        sliderInput("d_nprop",
                    "n: sample size",
                    min = 5,
                    max = 100,
                    step = 1,
                    value = 20,
                    animate=animationOptions(
                      interval = 100,
                      loop = TRUE,
                      playButton = "Play",
                      pauseButton = "Stop")
                    ),
        sliderInput("prop",
                    "p: probability of success",
                    min = 0.1,
                    max = 1,
                    step = 0.05,
                    value = 0.1,
                    animate=animationOptions(
                      interval = 100,
                      loop = TRUE,
                      playButton = "Play",
                      pauseButton = "Stop")
                    ) ,
        sliderInput("M",
                    "M: # of simulated p-hats",
                    min = 50,
                    max = 1000,
                    step= 50,
                    value = 50,
                    animate=animationOptions(
                      interval = 100,
                      loop = TRUE,
                      playButton = "Play",
                      pauseButton = "Stop")
                    )
      ),

      mainPanel(
        tabsetPanel(id = "proportion_example",
          tabPanel("Histogram",
                   plotOutput("hist")
                   ),
          tabPanel("CDF Plots",
                   plotOutput("cdf")
                   )#,
          # tabPanel("Absolute Difference in ECDF with Normal",  
          #          actionButton("three_d_plot", "Create/Update 3D Plot"),
          #          plotOutput("three_d")
          #          )
        )
      )
    )
  ),
  server <- function(input, output, session){
    
    sampled_values <- reactiveValues(sample_prop = c())
    observeEvent(c(input$d_nprop, input$prop, input$M),
                 sampled_values$sample_prop <- replicate(input$M, (rbinom(input$M,input$d_nprop, input$prop))/input$d_nprop)
                 )
    
    output$hist <- renderPlot({
      d_nprop <- isolate(input$d_nprop)
      data_prop <- sampled_values$sample_prop
      hist(data_prop, 
           main = paste0("Distribution of ",input$M," Observed Sample proportionss\nWith n = ", d_nprop),
           xlab = "Observed Sample Proportion",
           freq = FALSE)
          })
    
    output$cdf <- renderPlot({
      d_nprop <- isolate(input$d_nprop)
      prop <- isolate(input$prop)
      M <- isolate(input$M)
      data_prop <- sampled_values$sample_prop
      #plot the ecdf of the sample means
      plot(ecdf(data_prop), 
           main = paste0("Comparison of Empirical CDF to Best\nApproximating Normal CDF with Sample Size n = ", d_nprop),
           col = "red", 
           lwd = 2,
           xlab = "y/n",
           ylab = "CDF")
      #use theoretical normal here
      prop_mean <- prop
      prop_sd <- sqrt(prop*(1-prop)/d_nprop)
      x_plot <- seq(from = prop_mean-3*prop_sd, to = prop_mean+3*prop_sd, length = 500)
      #add the normal's cdf values
      lines(x = x_plot, 
            y = pnorm(x_plot, mean = prop_mean, sd = prop_sd),
            col = "blue", 
            lwd = 2)
      #add legend
      legend('topleft', 
             c(expression(hat(F[y/n](y))),expression(F[X](y))),
             col = c("red", "blue"),
             lwd= 2)
      })

    three_d_plot_values <- reactiveValues(plot_data = c())
    #create data for 3d plot
    observeEvent(input$three_d_plot, {
        d_nprop <- input$d_nprop
        prop <- input$prop
        M <- input$M
        ns <- 2:d_nprop
        #find means for each sample size
        props <- as.data.frame(
          #to every sample size value 2:n create 100 means of size n
          lapply(ns, 
                 FUN = function(x, M, prop){replicate(M, (rbinom(M, x, prop))/x)}, prop, M), 
          col.names = paste0("n",ns))
        #now find the ecdf for each sample size (each column)
        ecdfs <- apply(X = props, MARGIN = 2, FUN = ecdf)
        #gamma mean and sd
        prop_mean <- prop
        prop_sd <- sqrt(prop*(1-prop)/d_nprop)
        #create a sequence of ybars to evaluate the ecdf at
        ybars <- seq(from = prop_mean-3*prop_sd, to = prop_mean+3*prop_sd, length = 50)
        #find the corresponding normal distribution's cdf values
        zs <- as.data.frame(
          lapply(ns,
                 FUN = function(x, ybars, M,prop){
                   pnorm(ybars, 
                         mean = prop_mean, 
                         sd =  sqrt(prop*(1-prop)/d_nprop))
                   }, ybars = ybars,  M,prop),
          col.names = paste0("n", ns)
        )
        diffs <- lapply(X = 1:length(ecdfs), FUN = function(x) abs(ecdfs[[x]](ybars) - zs[,x]))
        #now we have the ns, ybars, and diffs to plot
        plot_data <- expand.grid(ybars = ybars, ns = ns)
        plot_data$diffs <- unlist(diffs) 
        three_d_plot_values$plot_data <- plot_data
    })
    
    output$three_d <- renderPlot({
        plot_data <- three_d_plot_values$plot_data
        # open3d()
        # plot3d(x = plot_data$ybars, y = plot_data$ns, z = plot_data$diffs)
        # scene <- scene3d()
        # close3d()
        # rglwidget(scene)
        if(length(plot_data)>0){
          print(lattice::wireframe(diffs ~ ybars + ns, 
                          data = plot_data, 
                          scales = list(arrows = FALSE), 
                          drape = TRUE, 
                          colorkey = TRUE, 
                          screen = list(z = -50, x = -70), 
                          zlab = list(expression(hat(l)[n] ~ "(x)=|" ~ hat(F)[n] ~ "(x)-" ~ F ~ "(x)|"), rot = 90), 
                          main = "Convergence in Distribution?", 
                          xlab="y/n"))
        } else {
          NULL
        }
    })
  },
  options(height = 5000))
```


#### CLT Applied to a Sum 

Recall the result: 

- If $X\sim N(\mu, \sigma^2)$ then $aX+b\sim N(a\mu+b,a^2\sigma^2)$

That is, a Normal random variable multiplied by a constant is still Normally distributed, just with a different mean and variance. 
- This tells us that if the CLT is applicable to the sample average, then we can also apply it to the corresponding summation as well!

Under the same assumptions as the CLT, since $n\bar{Y} = \sum_{i=1}^{n}Y_i$ we have the following result:
$$\frac{\sum_{i=1}^{n} Y_i -n\mu}{\sqrt{n}\sigma}\stackrel{\bullet}{\sim}N(0, 1)$$
or

$$\sum_{i=1}^{n} Y_i \stackrel{\bullet}{\sim}N(n\mu, n\sigma^2)$$

<hr>

**Example:** Based on this result, what is a good large-sample distribution for $Y\sim Bin(n, p)$?

<hr>


#### CLT Applied to a Gamma 

**Example:** Earlier we proved that $Y\sim Gamma(n, \lambda)$, properly standardized, converges to a standard normal distribution. That is, 
$$W = \frac{Y-n/\lambda}{\sqrt{n}/\lambda}\stackrel{d}{\rightarrow} Z \sim N(0,1)$$

Rather than use MGFs, we can apply the CLT in a clever way!

First, note that for $Y\sim gamma(n,\lambda)$ we can think of $Y$ as $Y=X_1+X_2+...+X_{n}$, where $X_i\stackrel{iid}\sim gamma(1,\lambda)$. Here we know that $E(X_i) = 1/\lambda$ and $Var(X_i) = 1/\lambda^2$. By the CLT applied to a sum we know

$$\frac{\sum_{i=1}^{n} X_i -n(1/\lambda)}{\sqrt{n}(1/\lambda)} = \frac{Y-n/\lambda}{\sqrt{n/\lambda^2}}\stackrel{\bullet}{\sim}Z$$

where $Z\sim N(0,1)$. That means we can approximate a gamma random variable by 
$$Y \stackrel{\bullet}{\sim} N(n/\lambda, n/\lambda^2)$$

Alternatively, we could apply the CLT to an average instead of a sum. Instead use $X_i\stackrel{iid}{\sim}gamma(1,\lambda/n)$ then $Y$ can be thought of as an average of these $X$'s
$$Y=\frac{1}{n}\sum_{i=1}^{n}X_i=\bar{X}\sim gamma(n,\lambda)$$
We know that $E(X_i) = n/\lambda$ and $Var(X_i) = n^2/lambda^2$. Since we are now viewing this as an average of iid random variables we can apply the CLT.  
$$\frac{\bar{X}-n\lambda}{\sqrt{(n^2/\lambda^2)/n}}=\frac{\bar{X}-n\lambda}{\sqrt{n/\lambda^2}}\stackrel{d}\rightarrow Z$$
where $Z\sim N(0,1)$.  


##### Convergence Exploration

- Suppose that $Y \sim Gamma(n, \lambda)$. Or, assume that $Y = \sum_{i=1}^{n} X_i$ where $X_i\stackrel{iid}\sim Gamma(1, \lambda)$. 
$$f_Y(y) = \frac{\lambda^\alpha}{\Gamma(\alpha)}y^{\alpha -1}e^{-\lambda y}$$
with mean $E(Y) = \alpha/\lambda$ and variance $Var(Y) = \alpha/\lambda^2$.
- We again showed we can approximate a gamma by a Normal distribution.  
- Can you develop a rule of thumb around $\alpha$ ($n$ here) and $\lambda$ for when a Normal distribution may be a reasonable approximation?  Remember we look for the following in each graph:  
    + Histogram: when does it become a symmetric bell-shape?
    + CDF comparison: When do the CDFs essentially overlap?

```{r, echo=FALSE}
shinyApp(
  ui <- fluidPage(
    sidebarLayout(
      sidebarPanel( 
        sliderInput("alpha",
                    "n: shape parameter",
                    min = 1,
                    max = 200,
                    step = 1,
                    value = 2,
                    animate=animationOptions(
                      interval = 100,
                      loop = TRUE,
                      playButton = "Play",
                      pauseButton = "Stop")
                    ) ,
        sliderInput("lambda",
                    "lambda: scale parameter",
                    min = 0.05,
                    max = 5,
                    step= 0.05,
                    value = 1,
                    animate=animationOptions(
                      interval = 100,
                      loop = TRUE,
                      playButton = "Play",
                      pauseButton = "Stop")
                    )
      ),

      mainPanel(
        tabsetPanel(id = "gamma_example",
          tabPanel("Gamma Density",
                   plotOutput("hist")
                   ),
          tabPanel("Gamma CDF",
                   plotOutput("cdf")
                   )#,
          # tabPanel("Absolute Difference in ECDF with Normal",  
          #          actionButton("three_d_plot", "Create/Update 3D Plot"),
          #          plotOutput("three_d")
          #          )
        )
      )
    )
  ),
  server <- function(input, output, session){
    
    output$hist <- renderPlot({
      alpha <- input$alpha
      lambda <- input$lambda
      x <- seq(0, to = alpha/lambda+ 4*sqrt(alpha/lambda^2), length = 1000)
      data_gamma <- dgamma(x, shape = alpha, rate = lambda)
      plot(x , data_gamma, type = "l", lwd = 2, main = paste0("Gamma(", alpha, ",", lambda, ") with N(", round(alpha/lambda,2), ",", round(alpha/lambda^2, 2), ")"), xlab = "gamma observation", ylab = "PDF", col = "blue")
      lines(x,
            dnorm(x, mean = alpha/lambda, sd = sqrt(alpha/lambda^2)), col = "red")
      legend("topright", legend = c("Gamma", "Normal"), col = c("blue", "red"), lwd = 1)
          })
    
    output$cdf <- renderPlot({
      alpha <- input$alpha
      lambda <- input$lambda
      x <- seq(0, to = alpha/lambda+ 4*sqrt(alpha/lambda^2), length = 1000)
      data_gamma <- pgamma(x, shape = alpha, rate = lambda)
      plot(x , data_gamma, type = "l", lwd = 2, main = paste0("Gamma(", alpha, ",", lambda, ") with N(", round(alpha/lambda,2), ",", round(alpha/lambda^2, 2), ")"), xlab = "gamma observation", ylab = "CDF", col = "blue")
      #add the normal's cdf values
      lines(x = x, 
            y = pnorm(x, mean = alpha/lambda, sd = sqrt(alpha/lambda^2)),
            col = "red", 
            lwd = 2)
      #add legend
      legend("topleft", legend = c("Gamma", "Normal"), col = c("blue", "red"), lwd = 1)
      })
  })
```


### CLT Importance 

Practically, why is the CLT so important? 

- The CLT gives us a distribution we can use to find probabilities when we deal with most sample sums and sample means.
- Knowing a large-sample distribution allows us to find (approximate) probabilities when exact probabilities may be too difficult to find.
- This means we can do approximate inference in many cases!

**Example:**

- Suppose we know $\sigma$ and we want inference for $\mu$.
- If we have a random sample $Y_1,...,Y_n$, we know  $\bar{Y}\stackrel{\bullet}{\sim}N(\mu,\sigma^2/n)$ ($\mu$ only unknown)
- We can make an approximate claim about $\mu$ via a confidence interval derived from an argument similar to that below:

\begin{align*}
P(-1.96<Z<1.96) &= 0.95\\
\Leftrightarrow P\left(-1.96<\frac{\bar{Y}-\mu}{\sigma/\sqrt{n}}<1.96\right) &= 0.95\\
\Leftrightarrow P\left(\bar{Y}-1.96\sigma/\sqrt{n}<\mu<\bar{Y}+1.96\sigma/\sqrt{n}\right) &= 0.95\\
\end{align*}

- That is, there is a 95\% probability the RVs
$\bar{Y}-1.96\sigma/\sqrt{n}$ and $\bar{Y}+1.96\sigma/\sqrt{n}$ capture $\mu$!
- In practice we observe a value for $\bar{Y}$ as $\bar{y}$. We then lose the ability to talk about probability but instead asy we are 95\% confident the observed interval contains $\mu$.
- Note: No assumption about $Y$'s distribution made other than finite variance!



## Convergence in Probability {.tabset}

### Definition 

We saw that the estimator of $p$, $\hat{p}$, from the Binomial example seemed to be observed closer and closer to $p$ for larger sample sizes. Additionally, we saw a good large-sample distribution for $\hat{p}$ is
$$\hat{p}\stackrel{\bullet}\sim N\left(p, \frac{p(1-p)}{n}\right)$$

Does this large-sample distribution support the 'convergence' of $\hat{p}$ to $p$ idea?  

More formally, we're going to take on the idea of **convergence in probability to a constant**. First, let's define convergence in probability generally.

Convergence in Probability
: A sequence of RVs $Y_1,...,Y_n,...$ converges in probability to a RV $Y$ if for every $\epsilon>0$
$$\lim_{n\rightarrow\infty}P(|Y_n-Y|\geq \epsilon)=0 \iff 
\lim_{n\rightarrow\infty}P(|Y_n-Y|<\epsilon)=1$$
This is denoted as 
$$Y_n\stackrel{p}{\rightarrow}Y$$

We'll mostly care about convergence in probability to a constant, call it $c$. We can see the definition in this case can be simplied to the following:
  $$\lim_{n\rightarrow\infty}P(|Y_n-c|< \epsilon)=\lim_{n\rightarrow\infty}P(-\epsilon < Y_n-c<\epsilon) =  \lim_{n\rightarrow\infty}P(c-\epsilon < Y_n<c+\epsilon)=1$$
$Y_n\stackrel{p}\rightarrow c$ if the *probability* we observe $Y_n$ close to $c$ goes to 1 in the limit.


**Example** - We can visualize this idea.

Assume that $Y_i\stackrel{iid}\sim N(0,1)$. Let's investigate the behavior of 
$$X = \frac{1}{n^2}\sum_{i=1}^{n}Y_i$$
To put this in the context of the definition, let's refer to $X$ explicitly as a function of $n$:
  $$X_n = \frac{1}{n^2}\sum_{i=1}^{n}Y_i$$
    We want to understand the behavior of $X_n$ as n grows. We'll see that $X_n\stackrel{p}\rightarrow 0$, which implies that for any $\epsilon>0$ we have
$$\lim_{n\rightarrow\infty}P(-\epsilon < X_n < \epsilon) =0$$

To visualize this, we can consider **sample paths** of $X_n$. That is, we can look at a particular sequence of $y_i$'s that will generate a sequence of $x$ and see how the values change. 
  
  Consider the following 6 values randomly sampled from a $N(0,1)$ and the corresponding sequence of $x_n$ values.
  ```{r, echo = FALSE}
  set.seed(1)
  ys <- rnorm(6)
  xn <- cumsum(ys)/((1:length(ys))**2)
  ```
  
  $y$ sequence      | $x$ sequence
  ------------------|--------------
    $y_1$ = `r ys[1]` | $x_1$ = `r ys[1]`$/1^2$ = `r xn[1]`
  $y_2$ = `r ys[2]` | $x_2$ = (`r ys[1]`+`r ys[2]`)$/2^2$ = `r xn[2]`
  $y_3$ = `r ys[3]` | $x_3$ = (`r ys[1]`+`r ys[2]`+`r ys[3]`)$/3^2$ = `r xn[3]`
  $y_4$ = `r ys[4]` | $x_4$ = (`r ys[1]`+...+`r ys[4]`)$/4^2$ = `r xn[4]`
  $y_5$ = `r ys[5]` | $x_5$ = (`r ys[1]`+...+`r ys[5]`)$/5^2$ = `r xn[5]`
  $y_6$ = `r ys[6]` | $x_6$ = (`r ys[1]`+...+`r ys[6]`)$/6^2$ = `r xn[6]`
  
  If we consider multiple sample paths, then convergence in probability to 0 of this sequence implies that the proportion of sample paths outside of $\pm \epsilon$ should go to zero.
  
  Let's plot our sample path with an $\epsilon = 0.05$:

```{r, echo = FALSE}
epsilon <- 0.05
n <- 6
plot_data <- data.frame(n = 1:n, xn = xn)
ggplot(plot_data, aes(x = n, y = xn)) + 
  geom_line() + 
  ylim(c(-0.75,0.75)) + 
  ggtitle("A sample path of Xn") + 
  geom_abline(intercept = -epsilon, slope = 0, color = "red") + 
  geom_abline(intercept = epsilon, slope = 0, color = "red") + 
  theme(legend.position = "none") +
  scale_x_continuous("n", breaks = 1:n, labels = as.character(1:n))
#plot(x = 1:n, 
#     y = xn, 
#     type = "l", 
#     main = "A sample path of X_n", 
#     ylim = c(-1, 1),
#     xlab = "n",
#     lwd = 2)
#abline(h = c(-epsilon, epsilon), col = "red", lwd = 2)
```

Now let's add 9 more sample paths:
    
```{r, echo = FALSE, warning = FALSE}
  #gen sample path function
  get_path <- function(n){
    xn <- cumsum(rnorm(n))/((1:n)^2)
  }
  M <- 10
  set.seed(1)
  paths <- replicate(M, get_path(n))
  plot_data <- data.frame(xn = c(paths), n = rep(1:n, times = M), path = as.factor(rep(1:M, each = n)))
  
  ggplot(plot_data, aes(x = n, y = xn, color = path)) + 
    geom_line()  + 
    ggtitle("A sample path of Xn") + 
    geom_abline(intercept = -epsilon, slope = 0, color = "red", size = 1.1, lty = "dashed") + 
    geom_abline(intercept = epsilon, slope = 0, color = "red", size = 1.1, lty = "dashed") + 
    annotate("rect", xmin = 3.75, xmax = 4.25, ymin = -0.75, ymax = 0.75, alpha = .2) +
    annotate("text", x = 4, y = 0.8, label = "7/10 fall within\nepsilon bounds", col = "darkblue") + 
    theme(legend.position = "none") +
    scale_x_continuous("n", breaks = 1:n, labels = as.character(1:n))
  
  ##add what xiaoxia has below with the bars and proportion business and ggploty this bitch 
  
  
  # plot(x = 1:n, 
  #      y = xn, 
  #      type = "l", 
  #      main = "A sample path of X_n", 
  #      ylim = c(-1, 1),
  #      xlab = "n", 
  #      lwd = 2)
  # for (i in 1:9){
  #   lines(x = 1:n, y = paths[,i], lwd = 2)
  # }
  # abline(h = c(-epsilon, epsilon), col = "red")
  
```
  
What we hope to see is that the proportion of lines falling outside of the $\epsilon$ bars goes to 0!
    
    
```{r, echo = FALSE, message = FALSE, warning = FALSE}
  #get matrix of probabilities for each n
  probability <- t(rowSums(abs(paths) > epsilon)/M)
  colnames(probability) <- paste0("X", 1:n)
  #produce data table and combind probability dataset
  table_data <- rbind(t(paths), probability)
  rownames(table_data) <- c(paste("Sample Path", 1:M), "Probability")
  round(table_data, 2) %>% 
    as.data.frame() %>%
    mutate_if(is.numeric, 
              function(x){
                x = cell_spec(x, 
                              color = ifelse(x >= 0.05, "red", "black"))
              }) %>%
    kable(escape = F, 
          row.names = T, 
          caption = "<center> Probability for 10 Simuations 
                  with sample size 6 </center>", 
          align = "l" ) %>%
    kable_styling(bootstrap_options = c("striped", "hover"), 
                  full_width = T) %>%
    row_spec(11, bold = T, color = "black", background = "lightblue")
```


**Example** - Suppose we have a random sample from a Normal distribution with mean 10 and standard deviation 1. What do you think $W = (\bar{Y})^2$ converges to in probability? Take an educated guess and use the app below to explore!
  
- Select the value c that you guess $W$ converges to in probability.
- Choose a sample size to go up to (start smaller and then get larger once you have a good idea).
- Select an $\epsilon$ range.
- Look for the proportion of lines (50 sample paths are generated) falling outside of the $\epsilon$ bars to go to 0!
  
```{r, echo=FALSE,message=FALSE, eval = TRUE}
#path for data
get_path <- function(n){
  xn <- (cumsum(rnorm(n, mean = 10, sd = 1))/(1:n))^2
}
M <- 50
shinyApp(
  ui <- fluidPage(
    #inputs on the side for n, p, and generating data
    sidebarPanel(
      numericInput("c",
                   "c: Value to converge to",
                   value = 0,
                   min = -500, 
                   max = 500),
      numericInput("n",
                   "n: Sample Size (1 to 1000)",
                   min = 1,
                   max = 1000,
                   step= 1,
                   value = 5),
      sliderInput("epsilon",
                  "epsilon: range",
                  min = 1,
                  max = 10,
                  step= 0.5,
                  value = 10),
      actionButton("create",
                   "Create/Update graph")
    ),
    mainPanel(
      plotlyOutput("convergence")
    )
  ),
  server <- function(input, output, session){
    ps <- reactiveValues(p = c())
    
    observeEvent(input$create, 
                 {
                   n <- input$n
                   #gen sample path function
                   paths <- replicate(M, get_path(n))
                   plot_data <- data.frame(xn = c(paths), 
                                           n = rep(1:n, times = M), 
                                           path = as.factor(rep(1:M, each = n)))
                   
                   ps$p <- plot_data
                 }
    )
    
    output$convergence <- renderPlotly({
      plot_data <- ps$p
      c <- isolate(input$c)
      n <- isolate(input$n)
      epsilon <- isolate(input$epsilon)
      
      if(length(plot_data) == 0) {
        NULL
      } else {
        #add proportion in bounds to data frame
        hover_data <- plot_data %>% 
          group_by(n) %>% 
          summarize(proportion = mean(abs(xn-c) < epsilon))
        if(n < 50){
          p <- ggplot(plot_data, aes(x = n, y = xn)) +
            geom_line(aes(color = path, group = path)) + 
            geom_hline(yintercept = c+epsilon, size = 1, col = "purple") + 
            geom_hline(yintercept = c-epsilon, size = 1, col = "purple") +
            theme_classic() + 
            theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = "none") +
            ggtitle("50 Sample Paths Visualized as the Sample Size Increases") + 
            ylab("Path Values") + 
            scale_x_continuous("n", breaks = 1:n, labels = as.character(1:n))
          p2 <- ggplotly(p)
          for (i in 1:n){
            p2 <- p2 %>% add_polygons(x = c(hover_data$n[i]-0.5, 
                                            hover_data$n[i]-0.5, 
                                            hover_data$n[i]+0.5,
                                            hover_data$n[i]+0.5), 
                                      y = c(-1000, 1000, 1000, -1000), 
                                      line = list(width = 0), 
                                      fillcolor = 'rgba(0, 0, 0, 0)', 
                                      inherit = FALSE, 
                                      name = paste0("Proportion of Sample Paths\n within epsilon bounds (n = ", i, "): ", hover_data$proportion[i]))
          }
        } else {
          p <- ggplot(plot_data, aes(x = n, y = xn)) +
            geom_line(aes(color = path, group = path)) + 
            geom_hline(yintercept = c+epsilon, size = 1, col = "purple") + 
            geom_hline(yintercept = c-epsilon, size = 1, col = "purple") +
            theme_classic() + 
            theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), legend.position = "none") +
            ggtitle("50 Sample Paths Visualized as the Sample Size Increases") + 
            ylab("Path Values")
          p2 <- ggplotly(p)
          for (i in c(seq(5, n, by = 10), n)){
            p2 <- p2 %>% add_polygons(x = c(hover_data$n[i]-5, 
                                            hover_data$n[i]-5, 
                                            hover_data$n[i]+5,
                                            hover_data$n[i]+5), 
                                      y = c(-500, 500, 500, -500), 
                                      line = list(width = 0), 
                                      fillcolor = 'rgba(0, 0, 0, 0)', 
                                      inherit = FALSE, 
                                      name = paste0("Proportion of Sample Paths\n within epsilon bounds (n = ", i, "): ", hover_data$proportion[i]))
          }
        }
        p2
      }
    })
  }
)


```

### Inequalities

To prove convergence in probability, we'll sometimes rely on some very famous inequalities. These will help us to show the probability goes to 0 or 1.

Markov's Inequality
: If $X$ is a nonnegative RV (support has no negative values) for which $E(X)$ exists, then for $t>0$
$$P(X\geq t)\leq \frac{E(X)}{t}$$

Example: If $X\sim exp(1)$ then $P(X\geq t)=e^{-t}$ and $E(X)/t=1/t$.

```{r, echo = FALSE, fig.align = 'center', out.width = "550px"}
x <- 1:120/20
plot(x, y = 1/x, type = "l", ylim = c(0,1), col = "blue", lwd = 2)
lines(x, y = exp(-x), type = "l", col = "green", lwd = 2)
legend("topright", legend = c("1/t", "exp(-t)"), col = c("blue", "green"), lwd = 2)
```


Chebychev's Inequality 
: Let $X$ be a RV with mean = $\mu$ and variance = $\sigma^2$, then for $t>0$ 
  $$P(|X-\mu|\geq t)\leq \frac{\sigma^2}{t^2}$$
  
Example:  If $t=\sigma k$ for $k>0$, we can apply Chebychev's to get
$$P\left(|X-\mu|\geq k\sigma\right)\leq \frac{\sigma^2}{k^2\sigma^2}=\frac{1}{k^2}$$
For $k=2$ we have $P\left(|X-\mu|\geq 2\sigma\right)\leq 1/4$.

Practically, what can we take home from this?

- At least 75\% of a RVs distribution lies within 2 standard deviations of the mean (if these moments exist)
- Regardless of distribution! (if moments exist)
- If $X\sim N(\mu,\sigma^2)$ we know $P(|X-\mu|\geq 2\sigma)\approx 0.05$. The bound isn't always very tight!
  

### WLLN

One of the most important results regarding convergence in probability is called the Law of Large Numbers (LLN).

(Weak) Law of Large Numbers (WLLN)
: Suppose $Y_i\stackrel{iid}\sim f$ where the mean and variance of $Y_i$ exist. Then $$\bar{Y}=\frac{1}{n}\sum_{i=1}^{n}Y_i\stackrel{p}\rightarrow E(Y)=\mu$$

- Big picture goal is to estimate parameters such as $\mu$
- If we get a RS we know that $\bar{Y}$ will be a 'close' to $\mu$ for `large' samples
- Applies to the **average of any independent random variables with the same finite mean**

Note that the variance assumption is actually not needed but will help us facilitate an easy proof. Let's use our inequalities to prove this result!



**Example** - Let $Y_i\stackrel{iid}{\sim}N(\mu, \sigma^2)$. What does $\bar{Y}$ converge to? What does $\frac{1}{n}\sum_{i=1}^{n}Y_i^2$ converge to?


### Continuity Theorems

The WLLN is also quite useful when combined with the continuity theorem.

Continuity Theorem
: If $Y_1,Y_2,Y_3,...$ converge to $Y$ and $g()$ is a continuous function then $g(Y_1),g(Y_2),g(Y_3)...$ converge to $g(Y)$. 

**Example (exploration example proved)** - 
Suppose we have a random sample from a Normal distribution with mean 10 and standard deviation 1. Consider $W = (\bar{Y})^2$. What does this converge to in probability?


Note: The continuity theorem also works for convergence in distribution!

**Example** - Suppose that $Y_i\stackrel{iid}\sim gamma(\alpha, \lambda)$. We have that 
$$\frac{\bar{Y}-\alpha/\lambda}{\frac{\sqrt{\alpha}}{\lambda\sqrt{n}}}\stackrel{d}\rightarrow Z$$
where $Z\sim N(0,1)$. By the continuity theorem we have that 
$$\left(\frac{\bar{Y}-\alpha/\lambda}{\frac{\sqrt{\alpha}}{\lambda\sqrt{n}}}\right)^2\stackrel{d}\rightarrow Z^2$$
and recall that a standard Normal squared is distributed as a $\chi^2_1$ or a $gamma(1/2, 1/2)$. 

#### Other Standard Limit Results Work Too!

Most of the common limit theorem ideas from calculus follow here as well ($\theta$ and $\lambda$ are constants below):

$$\mbox{If }Y\stackrel{p}{\rightarrow}\theta, X\stackrel{p}{\rightarrow}\lambda\mbox{ then }Y\pm X\stackrel{p}{\rightarrow}\theta\pm\lambda$$


**Example** - Consider the `biased' version of the sample variance, $S_n^2=\frac{1}{n}\sum_{i=1}^{n}(Y_i-\bar{Y})^2$.  Let's show $S_n^2\stackrel{p}{\rightarrow}\sigma^2$


### $\stackrel{d}\rightarrow$ & $\stackrel{p}\rightarrow$ Relationship

Convergence in probability implies convergence in distribution. However, the converse is not true generally (convegence in distribution does not imply convergence in probability). 

**Example** - Suppose $X\sim Beta(2,2)$ then $1-X$ is also distributed as Beta(2,2) (recall the symmetry of the Beta distribution with equal $\alpha$ and $\beta$).

Define a sequence of RVs to be $X_n=X$ for all $n$.  Then $X_n\stackrel{d}{\rightarrow}1-X\sim Beta(2,2)$.

Now consider convergence in probability, does $X_n\stackrel{p}{\rightarrow}1-X$?  No! To converge in probability we need 
$$P(|X_n-(1-X)|<\epsilon)=P(|X+X_n-1|<\epsilon)\rightarrow 1$$
as n goes to infinity for **every** $\epsilon>0$. Pick an $\epsilon$, say $\epsilon = 1/2$. As $X_n$ is defined as $X$, we have

\begin{eqnarray*}
P(|X+X_n-1|<\epsilon)=P(|2X-1|<\epsilon) & = & P((1-\epsilon)/2 < X < (1+\epsilon)/2) < 1\\
                     &= &P(1/4 < X < 3/4) <1
\end{eqnarray*}
for all $n$.

**Convergence in distribution to a constant** - If $Y_n\stackrel{d}{\rightarrow}c$ then $Y_n\stackrel{p}\rightarrow c$.

Why does it makes sense that convergence in distribution to a constant implies convergence in probability to that constant? Consider our example where we look at the maximum from a random sample of $U(0,1)$ RVs. Below are plots of the distribution of the sample max for varying $n$ values.

```{r echo= FALSE}
#######################################################################
#Generate many samples of size n from Uniform(0,1), find max for each sample to see the distribution of max
n<-c(3,10,100,10000)
N<-20000
maxes<-matrix(,nrow=N,ncol=length(n))

#loop over sample sizes
for (j in 1:length(n)){
  #loop through data sets
  for (i in 1:N){maxes[i,j]<-max(runif(n[j]))}
}

par(mfrow=c(1,length(n)))
hist(maxes[,1],breaks=seq(0,1,by=0.005),main=paste("Hist of Max, n=",n[1],sep=""),xlab="max",
ylab="Dist of max")
hist(maxes[,2],breaks=seq(0,1,by=0.005),main=paste("Hist of Max, n=",n[2],sep=""),xlab="max",
ylab="Dist of max")
hist(maxes[,3],breaks=seq(0,1,by=0.005),main=paste("Hist of Max, n=",n[3],sep=""),xlab="max",
ylab="Dist of max")
hist(maxes[,4],breaks=seq(0,1,by=0.005),main=paste("Hist of Max, n=",n[4],sep=""),xlab="max",
ylab="Dist of max")
par(mfrow = c(1,1))
```


Another really useful theorem relating convergence results is called Slutsky's Theorem.

Slutsky's Theorem
: If $X_n\stackrel{d}\rightarrow X$ and $Y_n\stackrel{p}\rightarrow a$, then 

- $X_nY_n\stackrel{d}\rightarrow aX$
- $X_n+Y_n\stackrel{d}\rightarrow X+a$


This theorem is extremely useful for creating hypothesis tests and confidence intervals! Recall the example we talked about when discussing the importance of the CLT:

**Example:**

- Suppose we know $\sigma$ and we want inference for $\mu$.
- If we have a random sample $Y_1,...,Y_n$, we know  $\bar{Y}\stackrel{\bullet}{\sim}N(\mu,\sigma^2/n)$ ($\mu$ only unknown)
- We can make an approximate claim about $\mu$ via a confidence interval derived from an argument similar to that below:

\begin{align*}
P(-1.96<Z<1.96) &= 0.95\\
\Leftrightarrow P\left(-1.96<\frac{\bar{Y}-\mu}{\sigma/\sqrt{n}}<1.96\right) &= 0.95\\
\Leftrightarrow P\left(\bar{Y}-1.96\sigma/\sqrt{n}<\mu<\bar{Y}+1.96\sigma/\sqrt{n}\right) &= 0.95\\
\end{align*}

- That is, there is a 95\% probability the RVs
$\bar{Y}-1.96\sigma/\sqrt{n}$ and $\bar{Y}+1.96\sigma/\sqrt{n}$ capture $\mu$!

Of course, $\sigma$ won't be known. Slutsky's theorem allows us to substitute a `consistent' estimator of $\sigma$ (i.e. an estimator of $\sigma^2$ that converges in probability to $\sigma$) and obtain a similar result!


### Delta Method

A common place where we'd use the CLT, LLN, and Slutsky's theorem together is when looking at **Delta Method Normality**.

**Large Sample Normality and the Delta Method**
: Let $Y_1,Y_2,...$ be a sequence of RVs such that $$\sqrt{n}(Y_n-\theta_0)\stackrel{d}{\rightarrow}N(0,\sigma^2)~~~~~~or~~~~Y_n\stackrel{\bullet}{\sim}N(\theta_0,\sigma^2/n)$$
For a function g and value $\theta_0$ where $g^{'}(\theta_0)$ exists and is not 0 we have
$$\sqrt{n}(g(Y_n)-g(\theta_0))\stackrel{d}{\rightarrow}N(0,(g^{'}(\theta_0))^2\sigma^2)~~~~~~or~~~~g(Y_n)\stackrel{\bullet}{\sim}N(g(\theta_0),(g^{'}(\theta_0))^2\sigma^2/n)$$

**Example** - Suppose $Y_i \stackrel{iid}\sim gamma(\alpha, \lambda)$. Goal: make inference on $\frac{1}{\mu}$.  Provide an approximate distribution for $1/\bar{Y}$ an \textbf{estimator} of $1/\mu$.


**Example** -  Let $Y_i\stackrel{iid}{\sim}Ber(p)$ then $\bar{Y}\stackrel{\bullet}{\sim}N(p,\frac{p(1-p)}{n})$.  Goal: make inference for $\frac{p}{1-p}$ using $\frac{\bar{Y}}{1-\bar{Y}}$.


**Example** - Suppose $Y_i\stackrel{iid}{\sim}N(\mu,\sigma^2)$ where $E(Y_i)=\mu\neq 0$.  Goal: make inference on $\frac{1}{\mu}$.  Provide 
an approximate distribution for $1/\bar{Y}$ an \textbf{estimator} of $1/\mu$.



## Recap

We have two big ideas:

- convergence in distribution
- convergence in probability

There are two big theorems:

- CLT
- WLLN

Strategies for proving convergence in distribution:

- CLT
- Delta Method Normality
- CDF convergence
- MGF convergence
- Convergence in probability implies convergence in distribution
- Continuity theorem applied to some result

Strategies for proving convergence in probability:

- LLN
- Continuity theorem
- Convergence in distribution to a constant implies convergence in probability
- Resort to the definition of convergence in probability and directly find the probability or use inequalities (Markov's or Chebychev's) 

```{r, echo = FALSE, fig.align='center', out.width = "800px"}
knitr::include_graphics("distr_relationship_wikipedia.jpg")
```

